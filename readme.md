## Отчет <a name="report"></a>

Это отчёт по задаче реализации метода WARP из статьи [WARP: On the Benefits of Weight Averaged Rewarded Policies](https://arxiv.org/pdf/2406.16768). Здесь мы посмотрим, как заставить модель генерировать тексты с каким-то определённым условием, как можно обучить её делать это при помощи WARP. Задача разбита на 3 части - обучение Reward модели, реализация WARP, проверка влияния гиперпараметров. Все они находятся в ноутбуках, но большая часть завёрнута в скрипты, которые лежат в папке `/warp`.

Я к сожалению не успел всё отдебажить точно к сроку. На работе нарушать дедлайны конечно не хорошо. Но я пытался до конца сделать хотя бы что-то, мне кажется это тоже важно. В конце концов есть очень много вопросов к статье, на которые было потрачено немало времени, которое в реальной задаче скорее всего было бы поменьше

### Как использовать
Все зависимости проекта находятся в `poetry.lock`, поэтому, если не лень, можно всё загрузить оттуда руками. Самые важные скрипты я специально продублировал в ноутбуках - искать ячейки, начинающиеся с `!poetry run`, они запускают все алгоритмы обучения с 0. Для более ленивых есть вариант скачать репозиторий, установить `poetry`, а следом все зависимости следующей командой
```bash
git clone https://github.com/Lerostre/rl-warp.git
cd rl-warp
pip install poetry
poetry lock --no-update
poetry install
```
Для работы в колабе нужен алгоритм посложнее. Например, добавить в ячейки следующие команды. Пример есть в `/notebooks`
```bash
!git clone https://github.com/Lerostre/rl-warp.git
%cd rl-warp
!git switch master
```
```bash
!pip install -q poetry
!poetry config virtualenvs.in-project true
!poetry lock --no-update
!poetry install
```

### Level 1. Reward modelling

По условию задачи нужно было обучить reward модель. Строго говоря, не уточняется, как это делать, но раз мы имеем дело с RL, я решил, что именно с ним и стоит повозиться. Нужно было собрать датасет из пар вида chosen-rejected, например,
| chosen | rejected |
| --- | --- |
| this movie is awful | I've never seen anything better in my life |

Далее - достаточно через RLHF обучить модель выбирать, какой из текстов лучше. Беда здесь в том, что в таком случае модель не научится оценивать reward одного текста, насколько я понимаю. Ей нужен будет референс

ЗАчем нам это нужно - в статье reward оценивается, как $r(x, y)$, где $x$ - какой-то промпт, а $y$ - его продолжение. Интерпретировать это можно двояко - либо, как reward нового предложения вместе с продолжением, тогда сойдёт любая моделька классификации, например, `distilbert-imdb`. Либо же это относительный reward между оригинальным предложением и его продолжением, тогда `RewardTrainer` даст именно то, что мы хотим, плюс в статье делается именно так

Для оценки относительного reward я позаимствовал функцию из того же `RewardTrainer`:
```python
    # Stack accepted against rejected, mean over logits
    # and softmax to get preferences between accepted and rejected to sum to 1
    logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T
```

`RewardTrainer` по итогу обучается, однако видно, что он бывает слишком уж категоричен - вероятности получаются большими.
Ещё из замечаний - reward в статье отрицательный

### Level 2. WARP

Вот тут уже гораздо сложнее. Для WARP нужны несколько вещей, в каждой из которых я в принципе мог налажать:
1. SLERP. Тут мне кажется, всё более-менее. Из нюансов, почему что-то могло пойти не так:
- мог неправильно определить многомерный случай
- мог неправильно определить матричный случай
Но такое чувство, что он сделан более-менее правильно
2. Policy. Это в принципе мутная тема. Я решил, что это вероятность всего предложения после такого-то промпта. В принципе значения, которые у меня получались, похожи на то, что было в тренере, так что тут я тоже скорее уверен. Но не факт. Доставал через кросс энтропию, потому что нужно делать это для модели отдельно, через одну лишь генерацию не получится
3. LIMA и прочие апдейты. Тут я делал немного костыльно, но такое чувство, будто в ручных апдейтах тоже всё в порядке. Я проверял, что параметры меняются и меняются на столько, сколько нужно
4. KL-div. Тут у меня вопросов много. Во-первых, та регуляризация, что в статье, вообще не похожа на дивергенцию. Я пытался найти другие источники про регуляризацию, но форма была везде такой. Почему это проблема - KL-дивергенция в целом не ограничена и $\beta$ мне не всегда поможет с ней справиться. Второе - она как будто бы бывает отрицательна, если я поменяю аргументы местами, что мега странно. Вот тут прям кажется, что я **неправ**. По итогу, чтобы не иметь с ней дел, я занулил везде бету
5. Reward. Сама моделька смущает - потому что RLHF даёт reward только в сравнении с чем-то, либо я неправильно понял. Ну и то, что реворд в статье отрицательный, это тоже как-то очень непонятно. К тому же не ясно, а он ограничен или нет? По статье как будто да, софтмаксом я это могу обеспечить, а надо ли? Тут мне тоже кажется, что ошибка могла быть

Ещё из важного - я взял поменьше примеров, не 100, а 50, потому что категорически не успевал. Я очень много времени потратил на дебаг, но так и не смог найти, в чём критическая ошибка. По итогу WARP учится, реворды меняются в нужную мне сторону, но вот сами тексты порой очень бредовые. Например, в них бывает повторяется одно слово, или одно и то же словосочетание, типа 'I was very dissapointed'. Возможно, это ожидаемое поведение, потому что это звучит, как лёгкий способ набить нужный (негативный) реворд.

### Level 3. Interpretation

Далее я хотел повторить хотя бы какую-то картинку из статьи, и выбор пал на параметр $\eta$. В принципе довольно прикольно, что тут, как и в случае с DPO и разными дивергенциями, есть трейдофф между KL (между чем и чем кстати?) и reward, то есть можно тонко настроиться под наши нужды

Тут я скорее снова потерпел фиаско, из причин могу придумать разве что недостаточное количество экспериментов: я не могу утверждать, что мои оценки достаточно робастны, т.к. примеров мало и моделей тоже

### Послесловие

Кажется, в анкете на этот раз не было, но мне всё равно хочется оставить какое-то сопроводительное. Это уже третье тестовое, которое я делаю, и к сожалению, на этот раз делаю плохо. 
Мне действительно нравится погружаться в статьи, на базе [GPICL](https://github.com/Lerostre/gpicl) у меня вырос диплом, на базе [DPO](https://github.com/Lerostre/test-task-alignment) - уже не моя курсовая, а позднее тоже будет диплом. 
Не могу сказать, что они тоже идеальны, однако ключевые результаты в них были, и отчёт там был красивее, и бонусы тоже были. На этот раз я кучу времени убил, чтобы создать красивую инфру, и это же по сути меня погубило, да и инфра получилась не настолько красивая. 
Просто обидно, что по сути я так и не знаю, почему не прошёл в прошлые разы, и был бы признателен фидбеку в этот раз, надеюсь, что приложил достаточно усилий для этого :(
