{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from scipy.stats import entropy\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "\n",
    "from scripts.utils.data import df_self_product\n",
    "from scripts.utils.misc import seed_everything\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at lvwerra/distilbert-imdb-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lvwerra/distilbert-imdb-cased\"\n",
    "\n",
    "reward_tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    model_name, max_length=512\n",
    ")\n",
    "reward_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lvwerra/gpt2-imdb\"\n",
    "\n",
    "sft_tokenizer = GPT2TokenizerFast.from_pretrained(model_name, max_length=512)\n",
    "sft_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train, test = [pd.DataFrame(dataset) for dataset in [train, test]]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = (\n",
    "    df_self_product(train, partition_col=\"label\")\n",
    "    .sample(10000)\n",
    "    .rename({\"text_0\": \"chosen\", \"text_1\": \"rejected\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 108\u001b[0m\n\u001b[1;32m     87\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     88\u001b[0m             dataset\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m     89\u001b[0m                 sample\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mfirst()\u001b[38;5;241m.\u001b[39mmap_batches(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcollect(streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_reward_dataset\u001b[39m(\n\u001b[1;32m    103\u001b[0m     examples: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m],\n\u001b[1;32m    104\u001b[0m     tokenizer,\n\u001b[1;32m    105\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m     max_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m    107\u001b[0m     truncation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m--> 108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mDataset\u001b[49m:\n\u001b[1;32m    109\u001b[0m     dataset_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m     ]\n\u001b[1;32m    115\u001b[0m     new_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    117\u001b[0m             dataset_keys,\n\u001b[1;32m    118\u001b[0m             [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset_keys))],\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from typing import Sequence, Literal, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def multi_join(\n",
    "    df_list: Sequence[pl.DataFrame | pl.LazyFrame],\n",
    "    on: Sequence[str | None] | str | None,\n",
    "    how: pl._typing.JoinStrategy = \"inner\",\n",
    ") -> pl.DataFrame | pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Join the list of pl.DataFrame of length N\n",
    "    using respective keys of length N-1 as `on`\n",
    "    with a given `how` method, common for all dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(on, Sequence):\n",
    "        on = [on] * len(df_list)\n",
    "\n",
    "    while len(df_list) > 1:\n",
    "        df_list[0] = df_list[0].join(df_list[1], on=on[0], how=how)\n",
    "        del df_list[1], on[1]\n",
    "        gc.collect()\n",
    "\n",
    "    return df_list[0]\n",
    "\n",
    "\n",
    "def sample_it(\n",
    "    s: pl.Series,\n",
    "    n_samples: int | float | None,\n",
    "    sample_mode: Literal[\"exact\", \"approximate\"] = \"exact\",\n",
    ") -> pl.Series:\n",
    "    \"\"\"\n",
    "    Custom pl.LazyFrame.sample implementation using shuffle\n",
    "    or binomial sampling techinques\n",
    "    \"\"\"\n",
    "    if isinstance(n_samples, float):\n",
    "        n_samples = int(s.len() * n_samples)\n",
    "    elif n_samples is None:\n",
    "        n_samples = s.len()\n",
    "\n",
    "    if sample_mode == \"exact\":\n",
    "        values = np.random.permutation(\n",
    "            np.hstack([np.ones(n_samples), np.zeros(s.len() - n_samples)])\n",
    "        )\n",
    "    elif sample_mode == \"approximate\":\n",
    "        values = np.random.binomial(1, n_samples / s.len(), s.len())\n",
    "\n",
    "    return pl.Series(\n",
    "        values=values,\n",
    "        dtype=pl.Boolean,\n",
    "    )\n",
    "\n",
    "\n",
    "def df_self_product(\n",
    "    dataset: pd.DataFrame | pl.DataFrame,\n",
    "    partition_col: str,\n",
    "    fields: Sequence[str] | str | None = None,\n",
    "    n_samples: int | float | None = None,\n",
    "    sample_mode: Literal[\"exact\", \"approximate\"] = \"approximate\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Dataframe self cross product of different columns with sampling if necessary\n",
    "    \"\"\"\n",
    "\n",
    "    if fields is None:\n",
    "        fields = dataset.columns\n",
    "\n",
    "    dataset = pl.DataFrame(dataset).partition_by(\n",
    "        partition_col, as_dict=True, include_key=False\n",
    "    )\n",
    "    dataset = [\n",
    "        dataset[key].select(pl.all().name.suffix(f\"_{key[0]}\")).lazy()\n",
    "        for key in dataset\n",
    "    ]\n",
    "    dataset = multi_join(dataset, on=fields, how=\"cross\")\n",
    "\n",
    "    # pl.LazyFrame has no efficient method of sampling,\n",
    "    # the block below is a placeholder for the future\n",
    "    if n_samples is not None:\n",
    "        dataset = (\n",
    "            dataset.with_columns(\n",
    "                sample=pl.first().map_batches(\n",
    "                    lambda x: sample_it(\n",
    "                        x, n_samples=n_samples, sample_mode=sample_mode\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            .filter(pl.col(\"sample\"))\n",
    "            .drop(\"sample\")\n",
    "        )\n",
    "\n",
    "    return dataset.collect(streaming=True)\n",
    "\n",
    "\n",
    "def prepare_reward_dataset(\n",
    "    examples: tuple[list],\n",
    "    tokenizer,\n",
    "    verbose: bool = False,\n",
    "    max_length: int = 512,\n",
    "    truncation: bool = True,\n",
    ") -> Dataset:\n",
    "    dataset_keys = [\n",
    "        \"input_ids_chosen\",\n",
    "        \"attention_mask_chosen\",\n",
    "        \"input_ids_rejected\",\n",
    "        \"attention_mask_rejected\",\n",
    "    ]\n",
    "    new_examples = dict(\n",
    "        zip(\n",
    "            dataset_keys,\n",
    "            [[0] * len(examples) for _ in range(len(dataset_keys))],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def tokenize_pair(idx):\n",
    "        tokenized_chosen = tokenizer(\n",
    "            examples[\"chosen\"][idx],\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        tokenized_rejected = tokenizer(\n",
    "            examples[\"rejected\"][idx],\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"][idx] = tokenized_chosen[\"input_ids\"]\n",
    "        new_examples[\"attention_mask_chosen\"][idx] = tokenized_chosen[\n",
    "            \"attention_mask\"\n",
    "        ]\n",
    "        new_examples[\"input_ids_rejected\"][idx] = tokenized_rejected[\n",
    "            \"input_ids\"\n",
    "        ]\n",
    "        new_examples[\"attention_mask_rejected\"][idx] = tokenized_rejected[\n",
    "            \"attention_mask\"\n",
    "        ]\n",
    "\n",
    "    iter_tuple = range(len(examples[\"chosen\"]))\n",
    "    if verbose:\n",
    "        iter_tuple = tqdm(iter_tuple)\n",
    "\n",
    "    Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "        delayed(tokenize_pair)(idx) for idx in iter_tuple\n",
    "    )\n",
    "    dataset = Dataset.from_dict(new_examples)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "reward_dataset = prepare_reward_dataset(sets, reward_tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-03 13:12:22.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_reward_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mStarting tokenizing `chosen`\u001b[0m\n",
      "\u001b[32m2024-08-03 13:12:26.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_reward_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mStarting tokenizing `rejected`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from typing import Sequence, Literal, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def multi_join(\n",
    "    df_list: Sequence[pl.DataFrame | pl.LazyFrame],\n",
    "    on: Sequence[str | None] | str | None,\n",
    "    how: pl._typing.JoinStrategy = \"inner\",\n",
    ") -> pl.DataFrame | pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Join the list of pl.DataFrame of length N\n",
    "    using respective keys of length N-1 as `on`\n",
    "    with a given `how` method, common for all dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(on, Sequence):\n",
    "        on = [on] * len(df_list)\n",
    "\n",
    "    while len(df_list) > 1:\n",
    "        df_list[0] = df_list[0].join(df_list[1], on=on[0], how=how)\n",
    "        del df_list[1], on[1]\n",
    "        gc.collect()\n",
    "\n",
    "    return df_list[0]\n",
    "\n",
    "\n",
    "def sample_it(\n",
    "    s: pl.Series,\n",
    "    n_samples: int | float | None,\n",
    "    sample_mode: Literal[\"exact\", \"approximate\"] = \"exact\",\n",
    ") -> pl.Series:\n",
    "    \"\"\"\n",
    "    Custom pl.LazyFrame.sample implementation using shuffle\n",
    "    or binomial sampling techinques\n",
    "    \"\"\"\n",
    "    if isinstance(n_samples, float):\n",
    "        n_samples = int(s.len() * n_samples)\n",
    "    elif n_samples is None:\n",
    "        n_samples = s.len()\n",
    "\n",
    "    if sample_mode == \"exact\":\n",
    "        values = np.random.permutation(\n",
    "            np.hstack([np.ones(n_samples), np.zeros(s.len() - n_samples)])\n",
    "        )\n",
    "    elif sample_mode == \"approximate\":\n",
    "        values = np.random.binomial(1, n_samples / s.len(), s.len())\n",
    "\n",
    "    return pl.Series(\n",
    "        values=values,\n",
    "        dtype=pl.Boolean,\n",
    "    )\n",
    "\n",
    "\n",
    "def df_self_product(\n",
    "    dataset: pd.DataFrame | pl.DataFrame,\n",
    "    partition_col: str,\n",
    "    fields: Sequence[str] | str | None = None,\n",
    "    n_samples: int | float | None = None,\n",
    "    sample_mode: Literal[\"exact\", \"approximate\"] = \"approximate\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Dataframe self cross product of different columns with sampling if necessary\n",
    "    \"\"\"\n",
    "\n",
    "    if fields is None:\n",
    "        fields = dataset.columns\n",
    "\n",
    "    dataset = pl.DataFrame(dataset).partition_by(\n",
    "        partition_col, as_dict=True, include_key=False\n",
    "    )\n",
    "    dataset = [\n",
    "        dataset[key].select(pl.all().name.suffix(f\"_{key[0]}\")).lazy()\n",
    "        for key in dataset\n",
    "    ]\n",
    "    dataset = multi_join(dataset, on=fields, how=\"cross\")\n",
    "\n",
    "    # pl.LazyFrame has no efficient method of sampling,\n",
    "    # the block below is a placeholder for the future\n",
    "    if n_samples is not None:\n",
    "        dataset = (\n",
    "            dataset.with_columns(\n",
    "                sample=pl.first().map_batches(\n",
    "                    lambda x: sample_it(\n",
    "                        x, n_samples=n_samples, sample_mode=sample_mode\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            .filter(pl.col(\"sample\"))\n",
    "            .drop(\"sample\")\n",
    "        )\n",
    "\n",
    "    return dataset.collect(streaming=True)\n",
    "\n",
    "\n",
    "def prepare_reward_dataset(\n",
    "    examples: dict[list],\n",
    "    tokenizer,\n",
    "    max_length: int = 512,\n",
    "    truncation: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Dataset:\n",
    "\n",
    "    logger.enable(\"__main__\") if verbose else logger.disable(\"__main__\")\n",
    "    token_kwargs = dict(\n",
    "        truncation=truncation,\n",
    "        max_length=max_length,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    new_examples = dict()\n",
    "\n",
    "    for texts in examples.keys():\n",
    "        logger.info(f\"Starting tokenizing `{texts}`\")\n",
    "        tokenized = tokenizer(text=examples[texts], **token_kwargs)\n",
    "        tokenized = {k + \"_\" + texts: v for k, v in tokenized.items()}\n",
    "        new_examples.update(tokenized)\n",
    "    dataset = Dataset.from_dict(new_examples)\n",
    "\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "reward_dataset = prepare_reward_dataset(\n",
    "    sets.to_dict(as_series=False), reward_tokenizer, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-03 13:12:53.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_reward\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mStarting tokenizing `text`\u001b[0m\n",
      "\u001b[32m2024-08-03 13:12:57.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_reward\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStarting reward estimation of `text`\u001b[0m\n",
      "\u001b[32m2024-08-03 13:13:27.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_reward\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mEstimating finished\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "what = get_reward(\n",
    "    pl.DataFrame(train).to_dict(as_series=False),\n",
    "    reward_tokenizer,\n",
    "    max_length=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model(\n",
    "    input_ids=torch.tensor(what[\"input_ids\"]).squeeze(1), attention_mask=torch.tensor(what[\"attention_mask\"]).squeeze(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(what[\"input_ids\"]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"../artifacts/reward_dataset.hf\"):\n",
    "    reward_dataset = reward_dataset.train_test_split(test_size=0.2)\n",
    "    reward_dataset.save_to_disk(\"../artifacts/reward_dataset.hf\")\n",
    "else:\n",
    "    reward_dataset = load_from_disk(\"../artifacts/reward_dataset.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dataset = reward_dataset.train_test_split(test_size=0.2)\n",
    "reward_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prompts(dataset, ):\n",
    "    dataset[\"text\"] = \n",
    "    imdb_prompts = [row[\"text\"].split(\".\")[0] for row in dataset[\"train\"] if row[\"label\"] == 1]\n",
    "    return np.random.choice(imdb_prompts, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import os\n",
    "\n",
    "peft_params = OmegaConf.load(\"../scripts/configs/peft_reward.yaml\")\n",
    "peft_config = LoraConfig(**peft_params)\n",
    "\n",
    "reward_trainer_params = OmegaConf.load(\"../scripts/configs/config_reward_trainer.yaml\")\n",
    "reward_config = RewardConfig(**reward_trainer_params)\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_config,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=reward_dataset[\"train\"],\n",
    "    eval_dataset=reward_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "conf = OmegaConf.load(\"../scripts/configs/reward_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.to_container(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
