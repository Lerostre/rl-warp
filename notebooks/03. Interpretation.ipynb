{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivanov.dko/projects/test/rl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "from warp.utils.misc import seed_everything\n",
    "from warp.utils.data import prepare_warp_dataset\n",
    "from warp.constants import DATASET_DIR, CONFIG_DIR, MODEL_DIR\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%cd ..\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этот раз нам нужны две модели. Одну из них мы предположительно обучили в прошлом ноутбуке. Если по какой-то причине она не лежит в `artifacts/reward_model`, можно скачать какую-то другую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = Path(MODEL_DIR, \"reward_model\")\n",
    "# model_name = \"lvwerra/distilbert-imdb\"\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, max_length=512, use_fast=True\n",
    ")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только оценить, насколько модель хороша. Одна у нас уже должна быть с прошлого ноутбука, если нет, то запустим ячейку ниже. В остальном нужно поварьировать параметр и оценить, насколько он вообще на что-то влияет. Я предлагаю взглянуть на $\\eta$, потому что у него есть прикольная зависимость от KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'warp_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "\u001b[32m2024-08-05 18:50:04.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mParsing WARP config\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:04.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mtrainer:\n",
      "  num_iterations: 2\n",
      "  num_runs: 2\n",
      "  num_steps: 100\n",
      "  mu: 0.01\n",
      "  lamb: 0.5\n",
      "  eta: 0.5\n",
      "  beta: 0.1\n",
      "  min_new_tokens: 15\n",
      "  temperature: 0.9\n",
      "dataset:\n",
      "  source: imdb\n",
      "  name: warp_dataset\n",
      "  test_size: 0.2\n",
      "  rewrite: true\n",
      "  max_length: 10\n",
      "  truncation: true\n",
      "dataloader:\n",
      "  batch_size: 64\n",
      "sft_model:\n",
      "  source: lvwerra/gpt2-imdb\n",
      "  name: lvwerra/gpt2-imdb\n",
      "  args:\n",
      "    local_files_only: false\n",
      "sft_tokenizer:\n",
      "  source: lvwerra/gpt2-imdb\n",
      "  args:\n",
      "    max_length: 512\n",
      "    use_fast: true\n",
      "reward_model:\n",
      "  source: distilbert/distilbert-base-cased\n",
      "  name: reward_model\n",
      "  args:\n",
      "    local_files_only: false\n",
      "reward_tokenizer:\n",
      "  source: distilbert/distilbert-base-cased\n",
      "  name: null\n",
      "  args:\n",
      "    max_length: 512\n",
      "    use_fast: true\n",
      "optimizer:\n",
      "  cls: torch.optim.Adam\n",
      "  kwargs:\n",
      "    lr: 1.0e-06\n",
      "scheduler:\n",
      "  cls: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR\n",
      "  kwargs:\n",
      "    warmup_epochs: 3\n",
      "    max_epochs: 100\n",
      "peft:\n",
      "  task_type: TaskType.CAUSAL_LM\n",
      "  inference_mode: false\n",
      "  r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.05\n",
      "  fan_in_fan_out: true\n",
      "seed: 42\n",
      "project: warp\n",
      "run_name: warp_model\n",
      "device: cpu\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:04.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoading SFT tokenizer from `/home/ivanov.dko/projects/test/rl/artifacts/models/lvwerra/gpt2-imdb`\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:05.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mLoading SFT model from `/home/ivanov.dko/projects/test/rl/artifacts/models/lvwerra/gpt2-imdb`\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:05.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Reward tokenizer from `/home/ivanov.dko/projects/test/rl/artifacts/models/distilbert/distilbert-base-cased`\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:05.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mLoading Reward model from `/home/ivanov.dko/projects/test/rl/artifacts/models/reward_model`\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:06.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mCreating `warp_dataset` dataset\u001b[0m\n",
      "\u001b[32m2024-08-05 18:50:14.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.data\u001b[0m:\u001b[36mprepare_warp_dataset\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mStarting tokenizing `text`\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|█| 25000/25000 [00:00<00:00, 172851.22 exa\n",
      "\u001b[32m2024-08-05 18:50:23.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mStarting WARPTrainer\u001b[0m\n",
      "[2024-08-05 18:50:23,663][bitsandbytes.cextension][WARNING] - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "Going through iterations:   0%|                           | 0/2 [00:00<?, ?it/s]\n",
      "Going through runs:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A/home/ivanov.dko/projects/test/rl/warp/structs/warp_trainer.py:114: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = self.scheduler(optimizer, **self.scheduler_kwargs)\n",
      "\n",
      "\n",
      "Going through steps:   0%|                              | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Going through steps:   1%|▏                     | 1/100 [00:11<19:41, 11.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   2%|▍                     | 2/100 [00:21<16:50, 10.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   3%|▋                     | 3/100 [00:28<14:20,  8.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   4%|▉                     | 4/100 [00:35<13:00,  8.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   5%|█                     | 5/100 [00:41<11:49,  7.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   6%|█▎                    | 6/100 [00:47<10:46,  6.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   7%|█▌                    | 7/100 [00:54<10:42,  6.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   8%|█▊                    | 8/100 [00:59<09:48,  6.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   9%|█▉                    | 9/100 [01:05<09:18,  6.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  10%|██                   | 10/100 [01:11<09:12,  6.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  11%|██▎                  | 11/100 [01:16<08:47,  5.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  12%|██▌                  | 12/100 [01:22<08:44,  5.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  13%|██▋                  | 13/100 [01:27<08:15,  5.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  14%|██▉                  | 14/100 [01:34<08:23,  5.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  15%|███▏                 | 15/100 [01:38<07:51,  5.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  16%|███▎                 | 16/100 [01:43<07:27,  5.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  17%|███▌                 | 17/100 [01:48<07:15,  5.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  18%|███▊                 | 18/100 [01:53<07:02,  5.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  19%|███▉                 | 19/100 [01:59<07:03,  5.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  20%|████▏                | 20/100 [02:04<06:57,  5.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  21%|████▍                | 21/100 [02:09<06:46,  5.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  22%|████▌                | 22/100 [02:14<06:35,  5.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  23%|████▊                | 23/100 [02:18<06:20,  4.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  24%|█████                | 24/100 [02:23<06:06,  4.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  25%|█████▎               | 25/100 [02:28<06:00,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  26%|█████▍               | 26/100 [02:32<05:52,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  27%|█████▋               | 27/100 [02:37<05:57,  4.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  28%|█████▉               | 28/100 [02:42<05:55,  4.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  29%|██████               | 29/100 [02:48<05:55,  5.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  30%|██████▎              | 30/100 [02:53<05:51,  5.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  31%|██████▌              | 31/100 [02:58<05:46,  5.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  32%|██████▋              | 32/100 [03:03<05:40,  5.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  33%|██████▉              | 33/100 [03:09<05:56,  5.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  34%|███████▏             | 34/100 [03:14<05:52,  5.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  35%|███████▎             | 35/100 [03:19<05:42,  5.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  36%|███████▌             | 36/100 [03:26<05:58,  5.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  37%|███████▊             | 37/100 [03:31<05:52,  5.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  38%|███████▉             | 38/100 [03:37<05:42,  5.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  39%|████████▏            | 39/100 [03:42<05:39,  5.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  40%|████████▍            | 40/100 [03:48<05:35,  5.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  41%|████████▌            | 41/100 [03:53<05:17,  5.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  42%|████████▊            | 42/100 [03:58<05:05,  5.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  43%|█████████            | 43/100 [04:03<04:57,  5.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  44%|█████████▏           | 44/100 [04:09<05:01,  5.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  45%|█████████▍           | 45/100 [04:14<04:54,  5.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  46%|█████████▋           | 46/100 [04:19<04:46,  5.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  47%|█████████▊           | 47/100 [04:24<04:36,  5.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  48%|██████████           | 48/100 [04:30<04:43,  5.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  49%|██████████▎          | 49/100 [04:36<04:45,  5.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  50%|██████████▌          | 50/100 [04:41<04:28,  5.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  51%|██████████▋          | 51/100 [04:46<04:21,  5.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  52%|██████████▉          | 52/100 [04:53<04:40,  5.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  53%|███████████▏         | 53/100 [04:59<04:30,  5.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  54%|███████████▎         | 54/100 [05:04<04:20,  5.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  55%|███████████▌         | 55/100 [05:11<04:27,  5.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  56%|███████████▊         | 56/100 [05:16<04:09,  5.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  57%|███████████▉         | 57/100 [05:22<04:06,  5.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  58%|████████████▏        | 58/100 [05:27<03:51,  5.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  59%|████████████▍        | 59/100 [05:32<03:46,  5.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  60%|████████████▌        | 60/100 [05:38<03:45,  5.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  61%|████████████▊        | 61/100 [05:43<03:34,  5.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  62%|█████████████        | 62/100 [05:49<03:29,  5.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  63%|█████████████▏       | 63/100 [05:53<03:04,  4.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  64%|█████████████▍       | 64/100 [05:56<02:38,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  65%|█████████████▋       | 65/100 [06:00<02:29,  4.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  66%|█████████████▊       | 66/100 [06:03<02:14,  3.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  67%|██████████████       | 67/100 [06:06<02:05,  3.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  68%|██████████████▎      | 68/100 [06:10<02:03,  3.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  69%|██████████████▍      | 69/100 [06:14<01:56,  3.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  70%|██████████████▋      | 70/100 [06:17<01:47,  3.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  71%|██████████████▉      | 71/100 [06:21<01:42,  3.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  72%|███████████████      | 72/100 [06:24<01:37,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  73%|███████████████▎     | 73/100 [06:27<01:32,  3.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  74%|███████████████▌     | 74/100 [06:30<01:28,  3.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  75%|███████████████▊     | 75/100 [06:34<01:25,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  76%|███████████████▉     | 76/100 [06:37<01:21,  3.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  77%|████████████████▏    | 77/100 [06:41<01:17,  3.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  78%|████████████████▍    | 78/100 [06:44<01:14,  3.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  79%|████████████████▌    | 79/100 [06:48<01:11,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  80%|████████████████▊    | 80/100 [06:51<01:08,  3.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  81%|█████████████████    | 81/100 [06:54<01:03,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  82%|█████████████████▏   | 82/100 [06:57<00:58,  3.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  83%|█████████████████▍   | 83/100 [07:01<00:56,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  84%|█████████████████▋   | 84/100 [07:04<00:53,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  85%|█████████████████▊   | 85/100 [07:08<00:51,  3.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  86%|██████████████████   | 86/100 [07:11<00:47,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  87%|██████████████████▎  | 87/100 [07:14<00:43,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  88%|██████████████████▍  | 88/100 [07:17<00:39,  3.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  89%|██████████████████▋  | 89/100 [07:21<00:36,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  90%|██████████████████▉  | 90/100 [07:24<00:32,  3.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  91%|███████████████████  | 91/100 [07:28<00:30,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  92%|███████████████████▎ | 92/100 [07:31<00:27,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  93%|███████████████████▌ | 93/100 [07:35<00:24,  3.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  94%|███████████████████▋ | 94/100 [07:38<00:21,  3.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  95%|███████████████████▉ | 95/100 [07:42<00:17,  3.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  96%|████████████████████▏| 96/100 [07:45<00:13,  3.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  97%|████████████████████▎| 97/100 [07:48<00:10,  3.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  98%|████████████████████▌| 98/100 [07:52<00:06,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  99%|████████████████████▊| 99/100 [07:55<00:03,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps: 100%|████████████████████| 100/100 [07:58<00:00,  4.79s/it]\u001b[A\u001b[A\n",
      "defaultdict(<class 'list'>, {'logps_theta': tensor(-18.0827), 'logps_ema': tensor(-18.0805), 'reward': tensor(-0.4408), 'reward_beta': tensor(-0.4406), 'loss': tensor(8.3044)})\n",
      "['I remember having a pretty low regard for a venture capitalist, and I was very disappointed. I was', 'This movie is a real gem. The arc of the story is so well done and the characters are', 'This film is not even worth walking to the movie theater. It is a waste of time and money', \"Not as bad as you've heard. There are some good moments, but the rest of the movie\", 'What are the odds of a \"Mermaid\" being a \"Mermaid\"?<br', 'I saw Chan Is Missing when it first came out, and I was very excited to see it.', '\\x08\\x08\\x08\\x08A Turkish Bath sequence in a movie.<br /><br />The movie is', 'La Chute de la Maison Usher, a film that is a bit of a disappointment.', 'I went to the movie theater this afternoon expecting to see a movie that was so bad it was funny', 'Wrapped in gorgeous English country backgrounds, Emma is a beautiful, beautiful woman who is always looking for', \"There's a legion of Mick Garris haters who are trying to make a mockery of the film\", 'Rachel, Jo, Hannah, Tina, Bradley and the rest of the cast. The film is a', 'I came to NEW PORT SOUTH expecting a good movie, but I was disappointed. I was', \"I am an avid fan of Lucio Fulci's work and I have seen many of his films\", 'Come on Tina Fey you can do better then this.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', 'John Cassavetes\\' \"Opening Night\" is a great example of how to make a film that', 'Lexi befriends Jennifer, a thin, intelligent, and very attractive young woman who is a very', 'This excellent drama had me in suspense the whole time. I was so excited to see this movie.', \"Soldier Blue is a movie with pretensions: it's a movie about a soldier who is sent\", 'What a dog of a movie. Noni Hazel is a very good actress, but she is not', 'Life Begins - and ends - in a typical 1930s-style film. The film is a bit', 'No message. No symbolism. No dark undercurrents. No suspense. No suspense. No suspense', \"Apparently Ruggero Deodato figured out, and he's a great actor. He's a\", 'Many of the criticisms on this thread seem to pick up on the fact that the movie is a bit', \"I won't repeat all that has been said already, but I will say that the movie is very\", 'While I loved this movie, the trailers that circulated were very good. I was expecting a lot of', 'I just rented (yes! I paid money to rent it) and I was so excited to see', 'The movie and acting are not bad and Jay Hernandez is a great actor. The movie is a little', 'After watching a dozen episodes, I decided to give it a try. I was pleasantly surprised. I', 'Says Andy: \"Nobody gets hurt, everybody gets hurt, and I\\'m not going to let', \"One of the more 'literate' Lone Stars, and one of the most underrated.<|endoftext|><|endoftext|>\", \"This is a very old and cheaply made film--it's not a good one. It's a\", \"SPOILER WARNING: There are some minor spoilers for this movie. I'm not going to spoil\", 'With the sun shining brilliantly on a quiet Sunday that is a perfect backdrop for the movie, the movie', 'Brazilian films often get more positive appraisals than negative ones. The film is a bit of', \"Based on Christy Brown's autobiographical novel, this film is a must see for anyone who has ever\", \"'Ninteen Eighty-Four' is a great movie. It's a great movie.\", 'This docu-drama is what you would expect from a movie about a man who is a', \"This might be the WWE's 2nd best PPV, but it's not the best PPV\", 'Despite a decent first season this series never came close to being a success. The first season was a', \"As some other comments show, this movie might scare you. It's not scary, it's not\", 'To like this movie at most you must be a fan of the original. I have seen it twice', 'I was looking forward to seeing Amanda Peet in this film. I was expecting a bit more of', 'Not even Bob Hope, escorted by a raft of other men, can save the day. The only', 'Every country which has a working film industry has some of the best films in the world. I think', 'The premise is amazing and the some of the acting is great. The story is very well told and', \"Although John Woo's hard Boiled is my number one favorite movie of all time, I think it\", 'I love this freekin movie! Walsh is a great actor and I love his acting. I love', \"Spoilers? Maybe a few details, but I think it's a good movie.<|endoftext|><|endoftext|>\", 'When i got this movie free from my job, i was so excited to see it. I was', 'I am a HUGE Adam Sandler fan, and I have seen many of his films, and I', 'A charming, funny film that gets a solid grade from the Academy.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', 'This movie was terrible to say the least. I was so disappointed. I was so disappointed. I', 'Errol Flynn is \"Gentleman Jim\" and he\\'s a good guy. He\\'s a', 'Michael Jackson is not very popular in USA anymore, but he is still a great actor. He is', 'The title of this film nearly put me off watching it. I was expecting something more than a little', 'I rented this movie the other night because neither my wife nor I were expecting anything. I was expecting', '\"Footlight Parade\" is fascinating on so many levels. It\\'s a film that is both funny', 'This is an Excellent little movie! The acting is great, the story is very well told, and', \"FAIL. I'd love to give this crap a 10.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", 'This film is something like a sequel of \"White Noise\" and \"The Last Man on Earth\".', 'Absolutely the worst film yet by Burton, who seems to have been a bit too much of a fan', 'Absolutely putrid slasher film has not one redeeming quality. The acting is atrocious, the', \"I can see little girls enjoying this show, but I'm not sure if it's a good idea\"]\n",
      "\n",
      "Going through runs:  50%|████████████            | 1/2 [07:59<07:59, 479.96s/it]\u001b[A\n",
      "\n",
      "Going through steps:   0%|                              | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   1%|▏                     | 1/100 [00:03<05:35,  3.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   2%|▍                     | 2/100 [00:07<05:52,  3.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   3%|▋                     | 3/100 [00:10<05:31,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   4%|▉                     | 4/100 [00:13<05:30,  3.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   5%|█                     | 5/100 [00:16<05:17,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   6%|█▎                    | 6/100 [00:20<05:16,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   7%|█▌                    | 7/100 [00:23<05:12,  3.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   8%|█▊                    | 8/100 [00:27<05:06,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   9%|█▉                    | 9/100 [00:30<05:08,  3.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  10%|██                   | 10/100 [00:33<04:59,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  11%|██▎                  | 11/100 [00:37<05:02,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  12%|██▌                  | 12/100 [00:41<05:18,  3.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  13%|██▋                  | 13/100 [00:45<05:23,  3.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  14%|██▉                  | 14/100 [00:48<05:15,  3.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  15%|███▏                 | 15/100 [00:52<05:08,  3.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  16%|███▎                 | 16/100 [00:56<05:03,  3.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  17%|███▌                 | 17/100 [00:59<04:56,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  18%|███▊                 | 18/100 [01:03<04:51,  3.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  19%|███▉                 | 19/100 [01:06<04:50,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  20%|████▏                | 20/100 [01:10<04:56,  3.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  21%|████▍                | 21/100 [01:14<04:52,  3.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  22%|████▌                | 22/100 [01:17<04:39,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  23%|████▊                | 23/100 [01:20<04:28,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  24%|█████                | 24/100 [01:24<04:30,  3.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  25%|█████▎               | 25/100 [01:27<04:19,  3.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  26%|█████▍               | 26/100 [01:31<04:17,  3.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  27%|█████▋               | 27/100 [01:34<04:11,  3.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  28%|█████▉               | 28/100 [01:38<04:16,  3.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  29%|██████               | 29/100 [01:42<04:29,  3.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  30%|██████▎              | 30/100 [01:46<04:17,  3.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  31%|██████▌              | 31/100 [01:50<04:14,  3.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  32%|██████▋              | 32/100 [01:53<04:04,  3.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  33%|██████▉              | 33/100 [01:56<03:57,  3.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  34%|███████▏             | 34/100 [02:00<03:56,  3.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  35%|███████▎             | 35/100 [02:03<03:45,  3.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  36%|███████▌             | 36/100 [02:06<03:36,  3.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  37%|███████▊             | 37/100 [02:10<03:34,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  38%|███████▉             | 38/100 [02:13<03:31,  3.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  39%|████████▏            | 39/100 [02:17<03:23,  3.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  40%|████████▍            | 40/100 [02:20<03:16,  3.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  41%|████████▌            | 41/100 [02:23<03:10,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  42%|████████▊            | 42/100 [02:26<03:14,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  43%|█████████            | 43/100 [02:30<03:08,  3.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  44%|█████████▏           | 44/100 [02:33<03:00,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  45%|█████████▍           | 45/100 [02:36<02:56,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  46%|█████████▋           | 46/100 [02:39<03:00,  3.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  47%|█████████▊           | 47/100 [02:43<02:59,  3.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  48%|██████████           | 48/100 [02:46<02:49,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  49%|██████████▎          | 49/100 [02:49<02:39,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  50%|██████████▌          | 50/100 [02:52<02:36,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  51%|██████████▋          | 51/100 [02:55<02:31,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  52%|██████████▉          | 52/100 [02:58<02:26,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  53%|███████████▏         | 53/100 [03:01<02:23,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  54%|███████████▎         | 54/100 [03:04<02:21,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  55%|███████████▌         | 55/100 [03:07<02:18,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  56%|███████████▊         | 56/100 [03:10<02:15,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  57%|███████████▉         | 57/100 [03:13<02:14,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  58%|████████████▏        | 58/100 [03:16<02:10,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  59%|████████████▍        | 59/100 [03:20<02:07,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  60%|████████████▌        | 60/100 [03:23<02:07,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  61%|████████████▊        | 61/100 [03:26<02:01,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  62%|█████████████        | 62/100 [03:29<01:58,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  63%|█████████████▏       | 63/100 [03:32<01:53,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  64%|█████████████▍       | 64/100 [03:35<01:54,  3.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  65%|█████████████▋       | 65/100 [03:39<01:52,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  66%|█████████████▊       | 66/100 [03:42<01:48,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  67%|██████████████       | 67/100 [03:45<01:46,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  68%|██████████████▎      | 68/100 [03:49<01:45,  3.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  69%|██████████████▍      | 69/100 [03:52<01:39,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  70%|██████████████▋      | 70/100 [03:55<01:34,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  71%|██████████████▉      | 71/100 [03:58<01:31,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  72%|███████████████      | 72/100 [04:01<01:30,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  73%|███████████████▎     | 73/100 [04:04<01:28,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  74%|███████████████▌     | 74/100 [04:08<01:23,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  75%|███████████████▊     | 75/100 [04:11<01:20,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  76%|███████████████▉     | 76/100 [04:14<01:16,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  77%|████████████████▏    | 77/100 [04:17<01:12,  3.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  78%|████████████████▍    | 78/100 [04:20<01:11,  3.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  79%|████████████████▌    | 79/100 [04:24<01:07,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  80%|████████████████▊    | 80/100 [04:27<01:02,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  81%|█████████████████    | 81/100 [04:30<01:00,  3.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  82%|█████████████████▏   | 82/100 [04:33<00:56,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  83%|█████████████████▍   | 83/100 [04:36<00:53,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  84%|█████████████████▋   | 84/100 [04:39<00:49,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  85%|█████████████████▊   | 85/100 [04:42<00:46,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  86%|██████████████████   | 86/100 [04:45<00:43,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  87%|██████████████████▎  | 87/100 [04:48<00:40,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  88%|██████████████████▍  | 88/100 [04:51<00:36,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  89%|██████████████████▋  | 89/100 [04:54<00:33,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  90%|██████████████████▉  | 90/100 [04:57<00:30,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  91%|███████████████████  | 91/100 [05:00<00:27,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  92%|███████████████████▎ | 92/100 [05:03<00:23,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  93%|███████████████████▌ | 93/100 [05:07<00:22,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  94%|███████████████████▋ | 94/100 [05:10<00:18,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  95%|███████████████████▉ | 95/100 [05:13<00:15,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  96%|████████████████████▏| 96/100 [05:16<00:12,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  97%|████████████████████▎| 97/100 [05:19<00:09,  3.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  98%|████████████████████▌| 98/100 [05:23<00:06,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  99%|████████████████████▊| 99/100 [05:26<00:03,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps: 100%|████████████████████| 100/100 [05:29<00:00,  3.29s/it]\u001b[A\u001b[A\n",
      "defaultdict(<class 'list'>, {'logps_theta': tensor(-18.3216), 'logps_ema': tensor(-18.3195), 'reward': tensor(-0.4404), 'reward_beta': tensor(-0.4402), 'loss': tensor(8.4469)})\n",
      "[\"SPOILER WARNING: There are some minor spoilers for this movie. I'm not going to spoil\", 'These two stars are the only iconic heroes/villains of the movie. The movie is a great', \"No, this hilariously horrible 70's made-for-TV movie is a waste of time.\", \"till HBO began rerunning it this month. I'm not sure if it's a good idea\", 'After coming off the first one you think the way he did it was a bit too much. The', \"I laughed a lot while watching this. It's a great movie.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", '\"Mr. Harvey Lights a Candle\" is anchored by a young, talented, and talented young man', \"A far as B-movies go, SCREAMING is a very good movie. It's\", 'If this movie was written directed and produced with the help of a few friends, I would have been', \"Dragon Hunters has to be the best-looking animated film ever made. It's a shame that it\", 'Manhattan apartment dwellers have to put up with the constant noise and the constant noise of the subway', 'Josef Von Sternberg directs this magnificent silent film. The film is a great example of the way', \"I was privileged to have seen some snippets from A.I. and the film, and I'm\", 'An allegation of aggravated sexual assault along with some other sexual assault is not enough to make this movie a', 'This was shown on the biography channel and was about the time of the first American war. It was', \"Having only seen two of his pictures previously, I can't help but think that this is one of\", \"The ship may have sunk but the movie didn't. The ship was so bad that it was almost\", 'This film is massively boring and pretentious. There are no plot twists, no suspense, no suspense', 'Chesty gringo Telly Savalas (who is also a great actor) and the rest', '***Comments contain spoilers*** I was barely holding on to my seat when I saw this movie. I', 'This is the only full length feature film about the film. It is a very good film, but', \"Well, I'd heard from somewhere that Ossession was a good movie, but I was not expecting\", 'The Brothers Quay are directors, judging by conventional wisdom, and they are not the only ones who', \"This film limps from self indulgent moment to self indulgent moment, and it's not even\", 'Yokai Monsters: Spook Warfare (Yokai Monsters: The Movie)<|endoftext|><|endoftext|><|endoftext|>', 'I am glad other people enjoyed this movie, cause I have seen it many times. I have seen', \"This is of of Sammo's great early comedy, and of his later work, and of his\", 'I really enjoyed the first episode and am looking forward to the next one.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', \"David Lean's worst film. Even 'In Which I'm a Woman' is a bad film.\", 'Sarah Silverman is really the \"flavor of the movie\" and the movie is a great example', 'I really enjoyed this movie. It succeeded in doing so. I think it was a great movie.', 'the costumes, the dialog, historical accuracy are terrible, and the acting is terrible. The only thing', \"I found 'Time At The Top' an entertaining movie. I'm not sure if it's a\", 'This film is unusual and bizarre, and it is a shame that it was not made in the USA', 'As a former 2 time Okinawan Karate world champion, I was very impressed with the performance of', 'The writers and producers of this little outing have plummeted to the bottom of the barrel. The film is', 'I have seen and liked the original film, and I have seen and liked the original film, and', \"Ummm, please forgive me, but weren't you expecting something like this? I mean, I\", 'As a lover of bad movies, I definitely hit the nail on the head with this one. I', \"Short, but long enough, Cat Soup is a great movie. It's a great movie, and\", 'If you made a genre flick in the late 80s, you might want to check out this one', 'This movie is a waste of time and money. It is a waste of time and money.<|endoftext|>', 'This outstanding Argentine independent film is one of the very best I have ever seen. It is a very', 'This film is a wonderfully simplistic work. Enjoyable, but not very well acted.<|endoftext|><|endoftext|><|endoftext|>', \"I really don't get all the adulation that I get from the people who have seen this movie\", 'Of course, the original is better, but this is a movie that is not worth watching.<|endoftext|>', \"This movie looked like a classic in the cheesy 80's. The acting was good, the story was\", 'I am a great fan of David Lynch and have seen his films, but I have never seen a', \"Wow, what a total let down! The fact that the movie was made in the early 90's\", \"Too bad Chuck Norris has gone to TV. He's a bad actor, and he's not even\", 'It was a bit bizarre and evil and i enjoyed it. I think it was a bit too much', \"I usually try to be professional and constructive when I write a review, but I just can't seem\", 'What was with all the Turkish actors? No offense to the Turkish actors, but they were not Turkish', 'This movie was not very entertaining, certainly NO WHERE near as good as the original. The acting was', 'This is the best piece of film ever created Its a great film, it is a great film,', 'From the critical acclaim, I expected more from this film. I was disappointed. I was expecting a', 'Wonderful film, one of the best horror films ever made.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', 'honestly, where can I begin! This movie is a total waste of time and money.', 'If somebody wants to make a really, REALLY bad movie, they should make a movie that is really', 'When I sat down to watch this film I actually thought it was a great film. I was wrong', \"For the life of me I can't understand the way this movie was made. I can't understand\", 'I was sooooo excited to see this movie after seeing it on TV. I was so excited to', \"A very strange and compelling movie. It's about a young man who is a very good actor and\", \"My comment is for the Russian version of Space Race, but I think it's a good movie.\"]\n",
      "\n",
      "Going through runs: 100%|████████████████████████| 2/2 [13:29<00:00, 404.72s/it]\u001b[A\n",
      "Going through iterations:  50%|█████████         | 1/2 [13:29<13:29, 809.88s/it]\n",
      "Going through runs:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Going through steps:   0%|                              | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   1%|▏                     | 1/100 [00:03<05:36,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   2%|▍                     | 2/100 [00:06<05:30,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   3%|▋                     | 3/100 [00:09<05:16,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   4%|▉                     | 4/100 [00:12<04:56,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   5%|█                     | 5/100 [00:15<04:58,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   6%|█▎                    | 6/100 [00:18<04:47,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   7%|█▌                    | 7/100 [00:21<04:41,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   8%|█▊                    | 8/100 [00:24<04:38,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   9%|█▉                    | 9/100 [00:27<04:32,  2.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  10%|██                   | 10/100 [00:30<04:29,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  11%|██▎                  | 11/100 [00:33<04:29,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  12%|██▌                  | 12/100 [00:36<04:24,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  13%|██▋                  | 13/100 [00:39<04:24,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  14%|██▉                  | 14/100 [00:43<04:22,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  15%|███▏                 | 15/100 [00:46<04:23,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  16%|███▎                 | 16/100 [00:49<04:20,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  17%|███▌                 | 17/100 [00:52<04:14,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  18%|███▊                 | 18/100 [00:55<04:11,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  19%|███▉                 | 19/100 [00:58<04:12,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  20%|████▏                | 20/100 [01:01<03:59,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  21%|████▍                | 21/100 [01:04<04:00,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  22%|████▌                | 22/100 [01:08<04:13,  3.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  23%|████▊                | 23/100 [01:11<04:19,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  24%|█████                | 24/100 [01:15<04:20,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  25%|█████▎               | 25/100 [01:18<04:09,  3.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  26%|█████▍               | 26/100 [01:21<03:58,  3.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  27%|█████▋               | 27/100 [01:25<04:02,  3.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  28%|█████▉               | 28/100 [01:28<03:59,  3.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  29%|██████               | 29/100 [01:31<03:48,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  30%|██████▎              | 30/100 [01:34<03:45,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  31%|██████▌              | 31/100 [01:37<03:41,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  32%|██████▋              | 32/100 [01:40<03:31,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  33%|██████▉              | 33/100 [01:44<03:33,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  34%|███████▏             | 34/100 [01:47<03:31,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  35%|███████▎             | 35/100 [01:50<03:28,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  36%|███████▌             | 36/100 [01:53<03:20,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  37%|███████▊             | 37/100 [01:56<03:19,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  38%|███████▉             | 38/100 [01:59<03:10,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  39%|████████▏            | 39/100 [02:02<03:04,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  40%|████████▍            | 40/100 [02:05<03:02,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  41%|████████▌            | 41/100 [02:08<02:58,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  42%|████████▊            | 42/100 [02:11<02:57,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  43%|█████████            | 43/100 [02:14<02:55,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  44%|█████████▏           | 44/100 [02:17<02:53,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  45%|█████████▍           | 45/100 [02:20<02:48,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  46%|█████████▋           | 46/100 [02:23<02:45,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  47%|█████████▊           | 47/100 [02:26<02:37,  2.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  48%|██████████           | 48/100 [02:29<02:36,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  49%|██████████▎          | 49/100 [02:32<02:33,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  50%|██████████▌          | 50/100 [02:35<02:30,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  51%|██████████▋          | 51/100 [02:39<02:32,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  52%|██████████▉          | 52/100 [02:42<02:30,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  53%|███████████▏         | 53/100 [02:45<02:24,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  54%|███████████▎         | 54/100 [02:48<02:22,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  55%|███████████▌         | 55/100 [02:51<02:19,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  56%|███████████▊         | 56/100 [02:54<02:16,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  57%|███████████▉         | 57/100 [02:58<02:18,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  58%|████████████▏        | 58/100 [03:01<02:17,  3.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  59%|████████████▍        | 59/100 [03:04<02:10,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  60%|████████████▌        | 60/100 [03:08<02:10,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  61%|████████████▊        | 61/100 [03:11<02:04,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  62%|█████████████        | 62/100 [03:13<01:55,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  63%|█████████████▏       | 63/100 [03:16<01:53,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  64%|█████████████▍       | 64/100 [03:20<01:51,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  65%|█████████████▋       | 65/100 [03:23<01:48,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  66%|█████████████▊       | 66/100 [03:26<01:47,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  67%|██████████████       | 67/100 [03:29<01:41,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  68%|██████████████▎      | 68/100 [03:32<01:35,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  69%|██████████████▍      | 69/100 [03:34<01:30,  2.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  70%|██████████████▋      | 70/100 [03:37<01:28,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  71%|██████████████▉      | 71/100 [03:41<01:27,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  72%|███████████████      | 72/100 [03:44<01:24,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  73%|███████████████▎     | 73/100 [03:47<01:21,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  74%|███████████████▌     | 74/100 [03:50<01:18,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  75%|███████████████▊     | 75/100 [03:53<01:15,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  76%|███████████████▉     | 76/100 [03:56<01:12,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  77%|████████████████▏    | 77/100 [03:59<01:09,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  78%|████████████████▍    | 78/100 [04:02<01:07,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  79%|████████████████▌    | 79/100 [04:05<01:03,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  80%|████████████████▊    | 80/100 [04:08<01:00,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  81%|█████████████████    | 81/100 [04:11<00:58,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  82%|█████████████████▏   | 82/100 [04:14<00:57,  3.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  83%|█████████████████▍   | 83/100 [04:17<00:51,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  84%|█████████████████▋   | 84/100 [04:20<00:49,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  85%|█████████████████▊   | 85/100 [04:24<00:48,  3.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  86%|██████████████████   | 86/100 [04:27<00:45,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  87%|██████████████████▎  | 87/100 [04:31<00:44,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  88%|██████████████████▍  | 88/100 [04:35<00:43,  3.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  89%|██████████████████▋  | 89/100 [04:39<00:40,  3.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  90%|██████████████████▉  | 90/100 [04:43<00:37,  3.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  91%|███████████████████  | 91/100 [04:47<00:34,  3.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  92%|███████████████████▎ | 92/100 [04:51<00:30,  3.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  93%|███████████████████▌ | 93/100 [04:54<00:26,  3.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  94%|███████████████████▋ | 94/100 [04:58<00:23,  3.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  95%|███████████████████▉ | 95/100 [05:02<00:19,  3.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  96%|████████████████████▏| 96/100 [05:06<00:15,  3.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  97%|████████████████████▎| 97/100 [05:10<00:11,  3.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  98%|████████████████████▌| 98/100 [05:14<00:07,  3.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  99%|████████████████████▊| 99/100 [05:19<00:04,  4.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps: 100%|████████████████████| 100/100 [05:22<00:00,  3.23s/it]\u001b[A\u001b[A\n",
      "defaultdict(<class 'list'>, {'logps_theta': tensor(-18.0311), 'logps_ema': tensor(-18.0290), 'reward': tensor(-0.4433), 'reward_beta': tensor(-0.4431), 'loss': tensor(8.3405)})\n",
      "['There is not a single sympathetic character in this entire movie. The only character that is sympathetic is the', 'I finally purchased and added to my collection a copy of the book. I was very disappointed. I', 'THE JIST: See something else.<br /><br />The movie is a bit slow,', 'Fatal Error is a really cool movie! Robert De Niro is great as the father of the', \"Some people might consider this movie a piece of artwork, but I think it's a very good piece\", \"Actually I liked this movie very, very much. I think it's a very good movie. I\", 'Shirley MacLaine in another tailor-made film, \"The Man Who Loved Me', '<br /><br />I really liked this movie. It was a great movie. I think', 'Okay, you have:<br /><br />1) The \"hero\" (the \"', \"I'm sure that not many people outside of Australia would be able to understand the story of the film\", 'I love this show!<br /><br />I love the show!<br /><', 'Ordinarily I really enjoy movies like \"Chances are, you\\'ll be disappointed\" and \"The', 'This movie is not your typical horror movie. It is a very good horror movie. It is a', 'Really touching story of a recruitment camp in America, and the story of a young man who is sent', '\"The Crush\" is a pleasant enough 40-minute movie, but it\\'s not a great movie', 'This movie was recently released on DVD in the US and I was very impressed with the quality of the', 'Originally I rented this film for my daughter since she was a little girl and I was very excited to', 'Myself and my groovadelic 20-year-old daughter, we were all so excited', 'You will marvel at the incredibly sophisticated computer animation, the beautiful cinematography, the beautiful music, the', \"This is possibly the worst version of the play I've ever seen. The acting is terrible, the\", 'Pickup on South Street (1953), directed by John Cusack, is a great example', 'This movie was awful and an insult to the viewer. I was so disappointed. I was so disappointed', \"This film is regarded by some as a classic - and I'm not saying that it's a masterpiece\", 'I have to say I quite enjoyed Soldier. Russell Crowe is a great actor and I think he', 'Poor Tobe Hopper. He directed an all-star cast of actors, including the great John', \"A very early Oliver Stone (associate-)produced film, and it's a shame that it's\", 'This is just a case of a previously worthless island. The island is a very small island, and', 'I must admit that this is the type of film that I have seen a lot of lately. I', \"With a relatively small budget for an animated film of this type, it's a shame that the film\", 'This episode introduced the Holodeck to the Troma family, and the family was very happy with', \"I'm a male, not given to women's roles, and I'm not a woman. I\", \"WOW! Why would anybody make a sequel to this movie? I mean, it's not like\", 'Ralph and Mumford, misfits in their own right, are the only ones who can save', \"One of the many Merrie Melodies cartoons that I've seen.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", 'A visit by Hitler in Rome is the backdrop of the film. The film is a very interesting and', \"Ah yez, the Sci Fi Channel produces Yet Another Movie, and I'm sure they'll be\", 'When the remake of When A Stranger Calls was out, I was very disappointed. I was expecting a', 'After the success of the second instalment, I was very disappointed. The first one was a', 'The Maxx is a deep psychological introspective lightly-edited, and very well-written, film', 'If the crew behind \"Zombie Chronicles\" ever get a chance to do something like this, they', 'Homelessness (or Houselessness as George Orwell would call it) is a film that is', 'Here is one the entire family will enjoy... even if it is a little too long.<|endoftext|><|endoftext|>', 'Well how was I suppose to know this was \"the\" movie? I was expecting something more than', 'Parker and Stone transplant their pacy expletives to the screen. The film is a bit', 'In her first nonaquatic role, Esther Williams plays a young woman who is in love with a', 'Dave is going through a divorce and his mind wanders. He is in love with a girl who', 'After seeing Jeremy Brett as Sherlock Holmes, no actor could have been more wrong. The film is a', 'Sweet romantic drama/comedy about Stewart and Sissy, a couple who are both married and have', 'BABY FACE is a fast paced, wise cracking thriller that is a must see for all fans of', 'The two things are are good about this film are the acting and the direction. The acting is good', 'If you ask me the first one was really better than the second one, I think it was.', 'This is a strong movie from a historical and epic perspective. It is a great movie to watch for', \"I wasn't going to watch this show. But I did. I was so excited to see this\", \"You'd think you're in for some serious sightseeing. But it's not. The scenery is\", 'Pegg has had a few hits in the past, but this one is the best of them.', 'On the night of his bachelor party, Paul Coleman (John Cusack) is a guest at', 'I was China in this film. I choose the film because it is a very good film. I', \"The story turns around Antonio 'Scarface' Montana, a young man who has been living in a\", \"A really sweet movie that has some similarities to the original. It's a very good movie, but\", 'This movie is the second worst film that I have ever seen. I have seen it twice and it', \"The Sentinel features a sort of run of the mill, but it's not as bad as the first\", \"I'm watching the series again now that it's over. I'm not sure if I'm going\", 'It does not seem that this movie managed to please the masses. It is a very boring movie.', 'Viewing \"Impulse\" is a very satisfying experience. The acting is excellent, the direction is']\n",
      "\n",
      "Going through runs:  50%|████████████            | 1/2 [05:23<05:23, 323.04s/it]\u001b[A\n",
      "\n",
      "Going through steps:   0%|                              | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   1%|▏                     | 1/100 [00:03<05:57,  3.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   2%|▍                     | 2/100 [00:07<05:47,  3.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   3%|▋                     | 3/100 [00:10<05:52,  3.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   4%|▉                     | 4/100 [00:15<06:10,  3.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   5%|█                     | 5/100 [00:19<06:16,  3.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   6%|█▎                    | 6/100 [00:22<05:41,  3.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   7%|█▌                    | 7/100 [00:25<05:22,  3.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   8%|█▊                    | 8/100 [00:28<05:13,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   9%|█▉                    | 9/100 [00:31<05:05,  3.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  10%|██                   | 10/100 [00:34<04:45,  3.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  11%|██▎                  | 11/100 [00:38<04:59,  3.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  12%|██▌                  | 12/100 [00:42<05:17,  3.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  13%|██▋                  | 13/100 [00:46<05:33,  3.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  14%|██▉                  | 14/100 [00:51<05:37,  3.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  15%|███▏                 | 15/100 [00:54<05:28,  3.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  16%|███▎                 | 16/100 [00:58<05:23,  3.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  17%|███▌                 | 17/100 [01:02<05:21,  3.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  18%|███▊                 | 18/100 [01:05<04:53,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  19%|███▉                 | 19/100 [01:09<04:55,  3.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  20%|████▏                | 20/100 [01:12<04:48,  3.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  21%|████▍                | 21/100 [01:15<04:32,  3.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  22%|████▌                | 22/100 [01:18<04:18,  3.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  23%|████▊                | 23/100 [01:21<04:07,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  24%|█████                | 24/100 [01:24<03:56,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  25%|█████▎               | 25/100 [01:27<03:49,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  26%|█████▍               | 26/100 [01:31<03:59,  3.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  27%|█████▋               | 27/100 [01:34<03:59,  3.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  28%|█████▉               | 28/100 [01:37<03:50,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  29%|██████               | 29/100 [01:40<03:41,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  30%|██████▎              | 30/100 [01:44<03:57,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  31%|██████▌              | 31/100 [01:48<04:09,  3.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  32%|██████▋              | 32/100 [01:52<04:10,  3.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  33%|██████▉              | 33/100 [01:55<03:59,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  34%|███████▏             | 34/100 [01:59<03:45,  3.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  35%|███████▎             | 35/100 [02:02<03:51,  3.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  36%|███████▌             | 36/100 [02:06<03:54,  3.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  37%|███████▊             | 37/100 [02:09<03:39,  3.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  38%|███████▉             | 38/100 [02:14<04:06,  3.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  39%|████████▏            | 39/100 [02:19<04:18,  4.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  40%|████████▍            | 40/100 [02:24<04:19,  4.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  41%|████████▌            | 41/100 [02:28<04:17,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  42%|████████▊            | 42/100 [02:33<04:11,  4.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  43%|█████████            | 43/100 [02:37<04:14,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  44%|█████████▏           | 44/100 [02:42<04:10,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  45%|█████████▍           | 45/100 [02:46<04:05,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  46%|█████████▋           | 46/100 [02:50<03:56,  4.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  47%|█████████▊           | 47/100 [02:55<03:50,  4.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  48%|██████████           | 48/100 [02:59<03:45,  4.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  49%|██████████▎          | 49/100 [03:04<03:49,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  50%|██████████▌          | 50/100 [03:08<03:41,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  51%|██████████▋          | 51/100 [03:13<03:36,  4.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  52%|██████████▉          | 52/100 [03:17<03:38,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  53%|███████████▏         | 53/100 [03:21<03:25,  4.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  54%|███████████▎         | 54/100 [03:26<03:22,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  55%|███████████▌         | 55/100 [03:30<03:20,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  56%|███████████▊         | 56/100 [03:35<03:20,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  57%|███████████▉         | 57/100 [03:40<03:16,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  58%|████████████▏        | 58/100 [03:45<03:15,  4.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  59%|████████████▍        | 59/100 [03:50<03:12,  4.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  60%|████████████▌        | 60/100 [03:54<03:06,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  61%|████████████▊        | 61/100 [03:59<03:04,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  62%|█████████████        | 62/100 [04:03<02:54,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  63%|█████████████▏       | 63/100 [04:08<02:47,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  64%|█████████████▍       | 64/100 [04:12<02:44,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  65%|█████████████▋       | 65/100 [04:17<02:40,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  66%|█████████████▊       | 66/100 [04:21<02:35,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  67%|██████████████       | 67/100 [04:26<02:30,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  68%|██████████████▎      | 68/100 [04:30<02:24,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  69%|██████████████▍      | 69/100 [04:35<02:20,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  70%|██████████████▋      | 70/100 [04:40<02:16,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  71%|██████████████▉      | 71/100 [04:44<02:10,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  72%|███████████████      | 72/100 [04:50<02:18,  4.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  73%|███████████████▎     | 73/100 [04:55<02:13,  4.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  74%|███████████████▌     | 74/100 [04:59<02:05,  4.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  75%|███████████████▊     | 75/100 [05:04<02:01,  4.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  76%|███████████████▉     | 76/100 [05:09<01:53,  4.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  77%|████████████████▏    | 77/100 [05:14<01:49,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  78%|████████████████▍    | 78/100 [05:18<01:41,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  79%|████████████████▌    | 79/100 [05:22<01:34,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  80%|████████████████▊    | 80/100 [05:27<01:31,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  81%|█████████████████    | 81/100 [05:31<01:25,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  82%|█████████████████▏   | 82/100 [05:36<01:23,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  83%|█████████████████▍   | 83/100 [05:40<01:14,  4.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  84%|█████████████████▋   | 84/100 [05:44<01:10,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  85%|█████████████████▊   | 85/100 [05:53<01:23,  5.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  86%|██████████████████   | 86/100 [05:59<01:22,  5.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  87%|██████████████████▎  | 87/100 [06:04<01:12,  5.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  88%|██████████████████▍  | 88/100 [06:09<01:04,  5.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  89%|██████████████████▋  | 89/100 [06:14<00:56,  5.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  90%|██████████████████▉  | 90/100 [06:18<00:49,  4.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  91%|███████████████████  | 91/100 [06:23<00:43,  4.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  92%|███████████████████▎ | 92/100 [06:27<00:37,  4.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  93%|███████████████████▌ | 93/100 [06:32<00:32,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  94%|███████████████████▋ | 94/100 [06:36<00:27,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  95%|███████████████████▉ | 95/100 [06:41<00:22,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  96%|████████████████████▏| 96/100 [06:45<00:18,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  97%|████████████████████▎| 97/100 [06:50<00:14,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  98%|████████████████████▌| 98/100 [06:54<00:09,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:  99%|████████████████████▊| 99/100 [07:01<00:05,  5.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps: 100%|████████████████████| 100/100 [07:05<00:00,  4.26s/it]\u001b[A\u001b[A\n",
      "defaultdict(<class 'list'>, {'logps_theta': tensor(-18.2438), 'logps_ema': tensor(-18.2418), 'reward': tensor(-0.4483), 'reward_beta': tensor(-0.4481), 'loss': tensor(8.5195)})\n",
      "[\"My cable TV has what's called the Arts channel, and I'm not sure if it's a\", 'THE NOTORIOUS BETTIE PAGE Written by Robert Altman, this is a very good', 'Henry Thomas was \"great\". His character held my attention for a long time. I was very impressed', \"I couldn't watch more than 14 minutes of it. I was so disappointed. I was so disappointed\", 'Wow. I do not think I have ever seen a movie that was so bad that it was so', 'Spoiler warning.<br /><br />When the movie starts, the main character is a young man', 'Tarzan and Jane are living happily in the jungle, and the two are in love. The two', \"Evening is an entertaining movie with quite some depth. It's a very good movie.<|endoftext|><|endoftext|>\", \"Ned aKelly is such an important story to me. I think it's a great movie.\", 'This is available on a \"Drive In Double Feature\" DVD.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', 'The Caprica episode (S01E01) is a great example of how the Caprica series', 'OK this movie had a terrible premise. Be serious, this movie is a waste of time.<|endoftext|>', \"THE HOUSE THAT DRIPPED BLOOD is the best of the best. It's a great movie\", \"Save the $8.97 you'll spend at the theater.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", 'How Rick Sloane was allowed to make five movies, he was a total waste of time.', \"Am not familiar with the trilogy but came upon this film in the early 90's. I was very\", \"This movie proves that you can't judge a movie by its plot, it's just a movie.\", 'As the film opens, two thugs kill another thug and then the two thugs are killed by the thugs', 'best movie ever!!!!! this movie broke my ribs just because i saw it. i was so excited to', 'A THIEF IN THE NIGHT is an excellent film. It is a great film to watch if', \"Whoo-boy, that was definitely one of the best movies I've seen in a long time\", 'Horrendous pillaging of a classic.<br /><br />The film is a bit of', \"Bette Midler is again Divine! Raunchy, sexy, and sexy. She's a\", 'The first in the series was brilliant, easily one of the best of the series. The second was', \"I'm no big fan of Martial Arts movies, but I'm a big fan of the martial arts\", 'A wonderful film by Powell and Pressburger, and a great film by the same name.<|endoftext|>', \"the movie is great, like every other international project I've seen. I'm not sure if it\", 'the author of the book, by the same title, is a very good writer. He is a', 'An entertaining and substantive film, Non-Stop has a great cast of actors and a great story.', 'This movie was so bad that my i.q. was forced to watch it. I was so', 'After watching about half of this I was ready to laugh. I was not. I was not laughing', 'First, let me state that I am a big fan of the original series. I have seen the', 'I thought it was a New-York located movie. I was wrong. I was wrong. I', 'I figured that any horror film with Orson Welles would have to be a horror film. I', 'This very good movie crackles with tension. The acting is good, the story is good, and', 'This is a wonderful film... First impressions of cynicism and the need for a moral compass are very strong', 'Spencer Tracy and Katherine Hepburn would roll in and out of the theater, and the movie', \"This film has a powerful philosophical ending. But that's not the point. The film is about the\", 'Tim Robbins is oddly benign here, cast as a young man who is in love with a woman who', 'Why take a show that millions of us watched and watched for a year and a half and you can', '<br /><br />I take issue with the fact that the film is set in the early', 'This film was the recipient of the 1990 Academy award for Best Picture. It was nominated for Best Picture', 'gone in 60 sec. where do i began, and what was the point of the movie?<', \"this movie has no plot, no character development, and no plot at all. It's just a\", \"I'm totally surprised by some of the comments on this site. I'm not a big fan of\", 'French production in which leading film directors from 11 countries are involved. The film is a very well made', 'The story is about a little girl growing up in a small town in the middle of nowhere. She', 'I recently watched the first Guinea Pig film, The Jungle Book, and I was very impressed with the', 'Since the start of her career in the 70\\'s, she was a star in the film \"The', 'This movie is completely ridiculous. Not only is the plot completely ridiculous, but the acting is so bad', 'Follow the Fleet, an RKO production in 1936, and the film\\'s first feature, \"The', \"This is the kind of movie that's so extremely boring and boring that it's hard to watch.\", 'The pakage implies that Warren Beatty and his wife, Mary, are the only ones who', 'Looking back over the past 28 years (since my first viewing of this film), I have seen a', \"This film is so much of a rip-off of the original, and I'm not sure if\", 'The information contained in this movie is somewhat familiar to me. I have seen the movie several times and', 'Far richer in texture and character than even the classics. The film is a great example of how to', 'The often-reliable Leonard Maltin says this is the best film he has ever seen. He', 'This film is the worst film, but it ranks as one of the worst films I have ever seen', 'I give this movie a 3 as it is worse than the first one. The acting is terrible,', 'This movie is plain fun.I has nothing to say about it,but I will say that it', \"I don't understand. Not being a critic, I can't understand why people would want to see\", 'This tale based on two Edgar Allen Poe pieces (\"The Lost World\" and \"The Lost City\")', 'This is one of the funniest movies that I have ever seen. It is a very funny movie']\n",
      "\n",
      "Going through runs: 100%|████████████████████████| 2/2 [12:28<00:00, 374.47s/it]\u001b[A\n",
      "Going through iterations: 100%|██████████████████| 2/2 [25:59<00:00, 779.59s/it]\n",
      "\u001b[32m2024-08-05 19:16:24.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mSaving model to `warp_model`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# default\n",
    "!poetry run python warp/train_warp.py --config-name warp_config run_name=warp_model trainer.eta=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'warp_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "\u001b[32m2024-08-05 20:02:13.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mParsing WARP config\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:13.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mtrainer:\n",
      "  num_iterations: 2\n",
      "  num_runs: 2\n",
      "  num_steps: 100\n",
      "  mu: 0.01\n",
      "  lamb: 0.5\n",
      "  eta: 0.1\n",
      "  beta: 0.1\n",
      "  min_new_tokens: 15\n",
      "  temperature: 0.9\n",
      "dataset:\n",
      "  source: imdb\n",
      "  name: warp_dataset\n",
      "  test_size: 0.2\n",
      "  rewrite: true\n",
      "  max_length: 10\n",
      "  truncation: true\n",
      "dataloader:\n",
      "  batch_size: 64\n",
      "sft_model:\n",
      "  source: lvwerra/gpt2-imdb\n",
      "  name: lvwerra/gpt2-imdb\n",
      "  args:\n",
      "    local_files_only: false\n",
      "sft_tokenizer:\n",
      "  source: lvwerra/gpt2-imdb\n",
      "  args:\n",
      "    max_length: 512\n",
      "    use_fast: true\n",
      "reward_model:\n",
      "  source: distilbert/distilbert-base-cased\n",
      "  name: reward_model\n",
      "  args:\n",
      "    local_files_only: false\n",
      "reward_tokenizer:\n",
      "  source: distilbert/distilbert-base-cased\n",
      "  name: null\n",
      "  args:\n",
      "    max_length: 512\n",
      "    use_fast: true\n",
      "optimizer:\n",
      "  cls: torch.optim.Adam\n",
      "  kwargs:\n",
      "    lr: 1.0e-06\n",
      "scheduler:\n",
      "  cls: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR\n",
      "  kwargs:\n",
      "    warmup_epochs: 3\n",
      "    max_epochs: 100\n",
      "peft:\n",
      "  task_type: TaskType.CAUSAL_LM\n",
      "  inference_mode: false\n",
      "  r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.05\n",
      "  fan_in_fan_out: true\n",
      "seed: 42\n",
      "project: warp\n",
      "run_name: warp_low_eta\n",
      "device: cpu\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:13.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoading SFT tokenizer from `/home/ivanov.dko/projects/test/rl/artifacts/models/lvwerra/gpt2-imdb`\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:13.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mLoading SFT model from `/home/ivanov.dko/projects/test/rl/artifacts/models/lvwerra/gpt2-imdb`\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:15.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Reward tokenizer from `/home/ivanov.dko/projects/test/rl/artifacts/models/distilbert/distilbert-base-cased`\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:16.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mLoading Reward model from `/home/ivanov.dko/projects/test/rl/artifacts/models/reward_model`\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:16.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mCreating `warp_dataset` dataset\u001b[0m\n",
      "\u001b[32m2024-08-05 20:02:24.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.data\u001b[0m:\u001b[36mprepare_warp_dataset\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mStarting tokenizing `text`\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|█| 25000/25000 [00:00<00:00, 220620.93 exa\n",
      "\u001b[32m2024-08-05 20:02:46.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_warp\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mStarting WARPTrainer\u001b[0m\n",
      "[2024-08-05 20:02:46,434][bitsandbytes.cextension][WARNING] - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "Going through iterations:   0%|                           | 0/2 [00:00<?, ?it/s]\n",
      "Going through runs:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A/home/ivanov.dko/projects/test/rl/warp/structs/warp_trainer.py:114: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = self.scheduler(optimizer, **self.scheduler_kwargs)\n",
      "\n",
      "\n",
      "Going through steps:   0%|                              | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ivanov.dko/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Going through steps:   1%|▏                     | 1/100 [00:06<11:12,  6.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   2%|▍                     | 2/100 [00:12<10:28,  6.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   3%|▋                     | 3/100 [00:19<10:08,  6.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   4%|▉                     | 4/100 [00:24<09:18,  5.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   5%|█                     | 5/100 [00:28<08:36,  5.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   6%|█▎                    | 6/100 [00:33<07:49,  4.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   7%|█▌                    | 7/100 [00:36<07:03,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   8%|█▊                    | 8/100 [00:40<06:30,  4.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "Going through steps:   9%|█▉                    | 9/100 [00:44<06:12,  4.10s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# eta=0.1\n",
    "!poetry run python warp/train_warp.py --config-name warp_config run_name=warp_low_eta trainer.eta=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta=0.9\n",
    "!poetry run python warp/train_warp.py --config-name warp_config run_name=warp_high_eta trainer.eta=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ivanov.dko/projects/test/rl/artifacts/models/warp_model were not used when initializing GPT2LMHeadModel: ['transformer.h.0.attn.c_attn.base_layer.bias', 'transformer.h.0.attn.c_attn.base_layer.weight', 'transformer.h.0.attn.c_attn.lora_A.default.weight', 'transformer.h.0.attn.c_attn.lora_B.default.weight', 'transformer.h.1.attn.c_attn.base_layer.bias', 'transformer.h.1.attn.c_attn.base_layer.weight', 'transformer.h.1.attn.c_attn.lora_A.default.weight', 'transformer.h.1.attn.c_attn.lora_B.default.weight', 'transformer.h.10.attn.c_attn.base_layer.bias', 'transformer.h.10.attn.c_attn.base_layer.weight', 'transformer.h.10.attn.c_attn.lora_A.default.weight', 'transformer.h.10.attn.c_attn.lora_B.default.weight', 'transformer.h.11.attn.c_attn.base_layer.bias', 'transformer.h.11.attn.c_attn.base_layer.weight', 'transformer.h.11.attn.c_attn.lora_A.default.weight', 'transformer.h.11.attn.c_attn.lora_B.default.weight', 'transformer.h.2.attn.c_attn.base_layer.bias', 'transformer.h.2.attn.c_attn.base_layer.weight', 'transformer.h.2.attn.c_attn.lora_A.default.weight', 'transformer.h.2.attn.c_attn.lora_B.default.weight', 'transformer.h.3.attn.c_attn.base_layer.bias', 'transformer.h.3.attn.c_attn.base_layer.weight', 'transformer.h.3.attn.c_attn.lora_A.default.weight', 'transformer.h.3.attn.c_attn.lora_B.default.weight', 'transformer.h.4.attn.c_attn.base_layer.bias', 'transformer.h.4.attn.c_attn.base_layer.weight', 'transformer.h.4.attn.c_attn.lora_A.default.weight', 'transformer.h.4.attn.c_attn.lora_B.default.weight', 'transformer.h.5.attn.c_attn.base_layer.bias', 'transformer.h.5.attn.c_attn.base_layer.weight', 'transformer.h.5.attn.c_attn.lora_A.default.weight', 'transformer.h.5.attn.c_attn.lora_B.default.weight', 'transformer.h.6.attn.c_attn.base_layer.bias', 'transformer.h.6.attn.c_attn.base_layer.weight', 'transformer.h.6.attn.c_attn.lora_A.default.weight', 'transformer.h.6.attn.c_attn.lora_B.default.weight', 'transformer.h.7.attn.c_attn.base_layer.bias', 'transformer.h.7.attn.c_attn.base_layer.weight', 'transformer.h.7.attn.c_attn.lora_A.default.weight', 'transformer.h.7.attn.c_attn.lora_B.default.weight', 'transformer.h.8.attn.c_attn.base_layer.bias', 'transformer.h.8.attn.c_attn.base_layer.weight', 'transformer.h.8.attn.c_attn.lora_A.default.weight', 'transformer.h.8.attn.c_attn.lora_B.default.weight', 'transformer.h.9.attn.c_attn.base_layer.bias', 'transformer.h.9.attn.c_attn.base_layer.weight', 'transformer.h.9.attn.c_attn.lora_A.default.weight', 'transformer.h.9.attn.c_attn.lora_B.default.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /home/ivanov.dko/projects/test/rl/artifacts/models/warp_model and are newly initialized: ['transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = Path(MODEL_DIR, \"lvwerra/gpt2-imdb\")\n",
    "model_name = Path(MODEL_DIR, \"warp_model\")\n",
    "\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name, max_length=512, use_fast=True\n",
    ")\n",
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, return_dict_in_generate=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценивать результаты, надо бы вообще завести отдельную функцию, стоило сделать сразу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_prompt(prompt):\n",
    "    completion = sft_tokenizer.batch_decode(\n",
    "        sft_model.generate(\n",
    "            temperature=0.9,\n",
    "            pad_token_id=sft_tokenizer.eos_token_id,\n",
    "            **sft_tokenizer([prompt], return_tensors=\"pt\")\n",
    "        ).sequences\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"completion\": completion[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'this movie is',\n",
       " 'completion': 'this movie is a man, and the way. I was a) and the way. I have'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_prompt(\"this movie is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5661,  3807,   318,   257,  7030,   286,   640,    13,   314,   373,\n",
       "           845, 11679,    13,   314,   373,   845, 11679,    13,   314,   373]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    sft_model.generate(\n",
    "        **sft_tokenizer([\"this movie is\"], return_tensors=\"pt\")\n",
    "    ).sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье указывается только про температуру, её мы пропишем. Уточняется, что они дообучали 28 слоёв, но в самой модели их чуть больше. Хотя они вроде не уточняют, что они брали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for param in sft_model.parameters():\n",
    "    if param.requires_grad:\n",
    "        i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_model, TaskType, LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, \n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=True\n",
    ")\n",
    "get_peft_model(sft_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for param in sft_model.parameters():\n",
    "    if param.requires_grad:\n",
    "        i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гораздо лучше, это и попробуем тюнить. Сразу скажу, что без адаптера я тоже пробовал, но модель довольно сильно плывём. Хочется думать, что основные её веса норм\n",
    "\n",
    "Следующий шаг - научиться находить полиси $\\pi(y|x)$. Насколько я понимаю, достаточно найти вероятность встретить каждый токен при наличии контекста, а затем перемножить. Это вообще можно достать из генерации, но прикол в том, что полиси нужно оценивать у двух разных моделей, для этого нужно похитрить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_policy(model, input_ids, len_generated):\n",
    "\n",
    "    logits = model(input_ids).logits\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "    policy = -F.cross_entropy(\n",
    "        shift_logits.transpose(1, 2), shift_labels, reduction=\"none\"\n",
    "    )[:, -len_generated:].sum(-1)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас в сетапе есть $\\theta$ и $\\theta_{\\text{ema}}$. У них один и тот же токенизатор, что гарантирует, что инпуты они обработают одинаково, это хорошо. Логиты можно достать, если прогнать форвард. Тогда останется посчитать кросс-энтропию от того, что мы нагенерили, получить то, что есть, то есть буквально $(x_n|x_1, ..., x_{n-1})$. Чтобы получить вероятность именно всего предложения, просуммируем логвероятности, как в обычной языковой модели. `len_generated` нужен, чтобы считать вероятность именно генерации. Вообще это костыль и его надо сделать красивее, потому что есть паддинг. Но в нашем сетапе длина везде 10, так что я забил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:270\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m sft_tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis movie is so\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m response_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43msft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m get_policy(response_input_ids, input_ids, \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/projects/test/rl/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1698\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1696\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1697\u001b[0m )\n\u001b[0;32m-> 1698\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1700\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/projects/test/rl/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:272\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_ids = sft_tokenizer([\"this movie is so\"], return_tensors=\"pt\")\n",
    "response_input_ids = sft_model.generate(input_ids, output_scores=True)\n",
    "get_policy(response_input_ids, input_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - научится делать интерполяцию. Начнём вот с чего:\n",
    "\n",
    "1. Методы интерполяции в статье делаются для векторов. Веса в сетках, к соэалению, обычно матрицы. Как этого избежать? Я не знаю, но я считал, что каждая матрица это кортеж векторов, каждый из которых я интерполирую\n",
    "2. SLERP делается для двух сущностей, а нам надо интерполировать много. В аппендиксе, если я правильно понял, они итеративно применяют его к каждой паре. Только применять его можно по-разному, например:\n",
    "$$\n",
    "\\theta_{\\text{init}} = \\theta_{\\text{init}} + \\lambda_1\\theta_{1} + \\lambda_2\\theta_{2} \\\\\n",
    "\\theta_{\\text{init}} = \\theta_{\\text{init}} + \\lambda_1(\\theta_{\\text{init}} + \\lambda_2\\theta_{2}) + ...\\theta_{3}\n",
    "$$\n",
    "То есть `slerp(slerp(theta, 1, 2), 3)`. Я же выбрал другой вариант, который, как по мне разумнее. Будем идти окном и мёрджить веса попарно. Изначальный вес добавим в самом конце\n",
    "$$\n",
    "\\theta_{2} = \\lambda_1\\theta_{1} + \\lambda_2\\theta_{2} \\\\\n",
    "... \\\\\n",
    "\\theta_{m} = \\lambda_1\\theta_{m-1} + \\lambda_2\\theta_{m} \\\\\n",
    "\\theta_{\\text{init}} = \\theta_{\\text{init}} + \\theta_{m}\n",
    "$$\n",
    "В общем, тут можно поразмышлять, как сделать лучше. Мне кажется, что это ни на что особо не повлияет, проверить к сожалению не успею, но вот идейка, как можно модернизировать\n",
    "\n",
    "3. Ручные обновления параметров надо сделать чуть покрасивее, чем это сделал сейчас я, тут каюсь, есть куда расти, но они лежат в тренере, тут я их показывать не буду\n",
    "\n",
    "Так что остаётся лишь взглянуть на SLERP и поедем тюнить модельку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slerp(theta, thetas, lamb):\n",
    "\n",
    "    for i in range(len(thetas) - 1):\n",
    "\n",
    "        delta_1 = thetas[i] - theta\n",
    "        delta_2 = thetas[i + 1] - theta\n",
    "\n",
    "        omega = (\n",
    "            torch.einsum(\n",
    "                \"ij, ij -> i\",\n",
    "                delta_1 / delta_1.norm(p=2, dim=1, keepdim=True),\n",
    "                delta_2 / delta_2.norm(p=2, dim=1, keepdim=True),\n",
    "            )\n",
    "            .unsqueeze(-1)\n",
    "            .arccos()\n",
    "        )\n",
    "\n",
    "        thetas[i + 1] = (\n",
    "            (torch.sin((1 - lamb) * omega) / torch.sin(omega)) * delta_1\n",
    "            + (torch.sin(lamb * omega) / torch.sin(omega)) * delta_2\n",
    "        )\n",
    "\n",
    "    return theta + thetas[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикольное упражнение на подумать - почему эта имплементация не очень, ответы в скрипте `warp/utils/train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.rand(3, 2)\n",
    "theta_1 = torch.rand(3, 2)\n",
    "theta_2 = torch.rand(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "slerped = slerp(theta, [theta_1, theta_2], lamb=0.5)\n",
    "assert slerped.shape == theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8703, 0.5297],\n",
       "         [0.1625, 0.7183],\n",
       "         [0.9897, 0.5562]]),\n",
       " tensor([[0.5833, 0.3161],\n",
       "         [0.8885, 0.7056],\n",
       "         [0.5693, 0.4831]]),\n",
       " tensor([[0.4092, 0.5345],\n",
       "         [0.4266, 0.4700],\n",
       "         [0.8235, 0.8636]]),\n",
       " tensor([[9.5895e-01, 2.4779e-01],\n",
       "         [8.0824e-04, 9.6473e-01],\n",
       "         [8.3183e-01, 1.9243e-01]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slerped, theta, theta_1, theta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, об адекватности судить сложно, но по крайней мере оно работает\n",
    "\n",
    "Осталось только собрать датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train, test = [pd.DataFrame(dataset) for dataset in [train, test]]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и сказано в задании, возьмём первые 10 токено, только и всего. Важно, что токенизировать их будем через SFT, потому что их же будем пихать в SFT-модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-05 17:48:23.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mwarp.utils.data\u001b[0m:\u001b[36mprepare_warp_dataset\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mStarting tokenizing `text`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'text'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "what = prepare_warp_dataset(\n",
    "    pl.DataFrame(train).to_dict(as_series=False),\n",
    "    tokenizer=sft_tokenizer,\n",
    "    max_length=10,\n",
    ")\n",
    "what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   40, 26399,   314,  3001,   327, 47269, 20958,    12,    56, 23304]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this movie is a waste of time. I was very disappointed. I was very disappointed. I was']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_tokenizer.batch_decode(\n",
    "    sft_model.generate(\n",
    "        temperature=0.9,\n",
    "        **sft_tokenizer([\"this movie is\"], return_tensors=\"pt\"),\n",
    "    ).sequences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, остаётся собрать трейн луп. Я бы с удовольствием сделал его как-нибудь красиво через lightning, но тут довольно много всего, так что внес в отдельный класс, который можно (и нужно) посмотреть в `warp/structs/warp_trainer.py`. Если я нигде не ошибся в реализации, то и сам алгоритм чисто теоретически должен работать. Запустить весь процесс можно командой ниже. Как обычно, рекомендую предварительно ознакомиться с параметрами в `warp/configs/warp_config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python warp/train_warp.py --config-name warp_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
