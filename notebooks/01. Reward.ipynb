{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast,\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "from warp.utils.misc import seed_everything\n",
    "from warp.utils.data import df_self_product, prepare_reward_dataset\n",
    "from warp.constants import DATASET_DIR, CONFIG_DIR, MODEL_DIR\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы нам понадобится всего одна модель, а именно берт. Поскольку у меня он есть локально, путей я указал тоже 2, при желании можно откатиться, линию я закомментил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /home/ivanov.dko/projects/test/rl/artifacts/models/distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = Path(MODEL_DIR, \"distilbert/distilbert-base-cased\")\n",
    "# model_name = \"distilbert/distilbert-base-cased\"\n",
    "\n",
    "reward_tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    model_name, max_length=512\n",
    ")\n",
    "reward_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет, как и просили, достанем из \"imdb\". Нам нужен только трейн, но это всё равно игрушечный ноутбук, так что не важно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train, test = [pd.DataFrame(dataset) for dataset in [train, test]]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее по условию нужно достать множество всех пар положительных и отрицательных отзывов. Для этого у меня есть отдельная функция. Должна работать быстро. Не должна крашиться по памяти, хотя там есть, что править. В частности все 125кк семплов нам не нужны, я взял лишь подмножество, и как это сделать эффективно - пока что вопрос. Подробнее в `warp.utils.data.df_self_product`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = (\n",
    "    df_self_product(train, partition_col=\"label\")\n",
    "    .sample(1000)\n",
    "    .rename({\"text_0\": \"chosen\", \"text_1\": \"rejected\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': ['I have two very major complaints regarding this film.<br /><br />1. That my local rental store shelved what is very clearly a soft core porn in the \"suspense\" category. (Had I known what it was, I would not have wasted my time renting it in the first place. And yes, this movie is a soft core porn.)<br /><br />2. The title has nothing to do with the movie. No one in this movie does anything that is either deviant or obsessive, let alone a combination of the two.<br /><br />Actually, make that three major complaints:<br /><br />3. That I for some reason watched the movie long enough to discover point number two on this list. Boy do I regret that. Stay away from this movie. Learn from my mistake. This movie is valueless on virtually every level.'],\n",
       " 'rejected': [\"I love this movie. My friend Marcus and I were browsing the local Hastings because we had an urge to rent something we had never seen before and stumbled across this fine film. We had no idea what it was going to be about, but it turned out spectacular. 2 thumbs up. I liked how the film was shot, and the actors were very funny. If you are are looking for a funny movie that also makes you think I highly suggest you quickly run to your local video store and find this movie. I would tell you some of my favorite parts but that might ruin the film for you so I won't. This movie is definitely on my top 10 list of good movies. Do you really think Nothing is bouncy?\"]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets.sample(1).to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, как мы видим метки немножко перепутались, но так даже прикольнее, мы же не обсуждали, какой именно alignment мы хотим. Правится это элементарно - у меня выше есть `rename`\n",
    "\n",
    "Дальше нам нужно подготовить датасет. Там всё просто - токенизируем каждый пример и пишем в датасет. Примеры функции уже есть в `huggingface`, но я её немножко модернизировал. Параллелить смысла нет, быстрый токенайзер уже действительно быстрый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-05 13:23:25.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mwarp.utils.data\u001b[0m:\u001b[36mprepare_reward_dataset\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mStarting tokenizing `chosen`\u001b[0m\n",
      "\u001b[32m2024-08-05 13:23:25.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mwarp.utils.data\u001b[0m:\u001b[36mprepare_reward_dataset\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mStarting tokenizing `rejected`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_dataset = prepare_reward_dataset(\n",
    "    sets.to_dict(as_series=False), tokenizer=reward_tokenizer\n",
    ")\n",
    "reward_dataset = reward_dataset.train_test_split(test_size=0.2)\n",
    "reward_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids_chosen': tensor([  101,   146,  1138,  1309,  2542,   170,  2523,  1107,  1177,  1376,\n",
       "          1159,   119,  1109,  1178, 19718,  1108,  1103,  2698,  1977,  3053,\n",
       "          1113,  1103,  4173,  2587,   119,  1135,  1108,  1176,  2903,   170,\n",
       "          9874,  1666, 13979,  1273,   119,  1247,  1108,  5544,  1185,  1948,\n",
       "          1111,  7570,   117,  7973,   117,  3741,   117,  2450,   117,  5444,\n",
       "         13814,  1116,   117, 11884,   117,  5681,   119,   119,   119,  2048,\n",
       "           117,  1175,  1108,  7284,  1185,  1642,  1719,   106,   146,  1444,\n",
       "          1106,  3593,  1995,  2442,  1104,  7368,   119,   119,   119,  6304,\n",
       "           117,  9684,   117,  9210,   117,   192,  8127,  6428,   117, 25096,\n",
       "          1158,   117,  1106,  3740,  8163,   117, 16516,  9436,  1361,   117,\n",
       "          9468,  5613,  2285,   117,  4809,  4777,   117,  1121,  2553,   117,\n",
       "         22852,   117, 22593, 25936,  1183,   117,   178, 23143,   117,   194,\n",
       "          8474,  1183,   117, 23609,  9144,   117,   188,  6105,  3781,   117,\n",
       "          4773,  1183,   117,   191,  4759,   117,  1508, 10132,   117,  1155,\n",
       "           118, 18680,   118,  1205,   117,  1105,   146,  1274,   112,   189,\n",
       "          1221,  1191,   146,  1169,  1712,  1113,  1280,  1106,  2335,  1155,\n",
       "          1995,  2442,  1104,  1198,  1293,  2213,  1142,  2727,  1104, 11074,\n",
       "           118,   184,   118, 22591,  1596,  1707,  1108,   106,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask_chosen': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'input_ids_rejected': tensor([  101,   146,  1486,  1142,  2523,  1103,  1285,  1122,  1338,  1149,\n",
       "          1314,  1214,   119,  8790,  5815,  4179,   146,  1354,   119,  2119,\n",
       "           117,  1208,  1122,   112,   188,  1113,  1888,  1105,   146,  1486,\n",
       "          1122,  1254,   119,   146,  1567,  1142,  2523,   106,  1109,  1614,\n",
       "          1152,  1202,  1132,  2121, 14908,  1133,  1115,   112,   188,  1184,\n",
       "          2228,  1122,  1139,  1503,  5095,  2523,  1104,  1155,  1159,   119,\n",
       "          1109,  1957,  3154,  1132,  3008,   117,  1133,  1103, 20787,  2340,\n",
       "         17693,  8032,  1209,  1138,  1128,  6362,   119,   146,   112,   182,\n",
       "          1103,  1912,  1104,  1825,  1115,   112,  1325,  1474,   178,   112,\n",
       "           182,  3768,  1118,  1142,  2523,   117,  1177,  1191,  1128,  1176,\n",
       "         18282,  1105,  1168,  4333,   117,  3644,   119,  1252,  1111,  1155,\n",
       "          1639,   117,  5548,   106,  1109,  3176,  1110, 25876,   119,  8902,\n",
       "           138,  8950,  1465,  1110,  1493,  1205,  1103,  1436,   113,  1119,\n",
       "           112,   188,  4534,   170,  3254,  9745,   117,  4040,   170,  5735,\n",
       "           114,  1723,  1118,  3096,  4209,  1200,   113, 14863,   117,  1274,\n",
       "           112,   189,  5663,  1143,   119,  1122,  4809,  5026,  1143,   114,\n",
       "          1105,  1173,  1613,   145,   119, 22957, 13232,  1117,  1436,  2099,\n",
       "           113, 24046,  8265,  1158,  1677,  2758,   114, 14325,  1144,  5185,\n",
       "          2490,  1121, 23639,  1183,  1106,   176,  4626,  8057,  2858,   117,\n",
       "          1133,   146,  1341, 26835,  1233,  5741,  1108,  2785,  1363,  1112,\n",
       "         24823,  4596,   119,  1960,  2645,   131,  1109,  1211, 12533,  1226,\n",
       "          1104,  1103,  1273,  1110,  1103,  4841,  1643,  7841,  1104,  1103,\n",
       "          9688,  1206,  4209,  1200,  1105,  5584,  1370, 21690,   117,  1105,\n",
       "          1103, 17154, 14570, 21122,  4510,   119,  1789,  4429,  7284,  4657,\n",
       "          1103,  1642,  1107,  1185,  1236,   117,  1133,  1152,   112,  1231,\n",
       "           170,  9232,   119, 14477, 22294,  1105,  2108,  1103,  5094,  6613,\n",
       "          1106,  8194,  1113,   170,  2741,  1235,  1233,  1155,  1122,   112,\n",
       "           188, 20844,  5815,  1785,  1110,  2065,   117,  1133,   171,  2312,\n",
       "          1152,  6878,  1105,  1128,   112,  1231,  2407,  1111,  1167,   119,\n",
       "           146,  8222,  1170,  3195,  1142,   117,  1128,  1209,  1129,  4871,\n",
       "          1121,  1103, 12154, 17396,   113,  1134,   146,  1341,  1108,  2785,\n",
       "          4348,   114,  1109,  4504,  1110,  2785,  4348,  1145,   117,  2232,\n",
       "          1120,   170, 10927,  6418,  1114,  1103,  5681,   119,  2907,   117,\n",
       "          2545,  7911,  1116, 19603,  1126,  6976,  2099,   113,  1119,  1144,\n",
       "          1142, 21884,  4348,  2213,  6485,  4348,  1757,  1106,  1140,   114,\n",
       "          1105,  1176,  1800,  1950,  1163,   117,  1103,  1436,  2192,  1132,\n",
       "          1165,  1103,  2650,  1437,  1199,  1769,  1757,  1106,  1172,   119,\n",
       "          2791, 16035,  1110,  2785,  6276,   117,   113,  2108,  1117,  4055,\n",
       "          1106, 17154, 14570,  1164,  1117,  3264,  2197,   118,   146,  1108,\n",
       "          6362,   114,  1105,  6274,  1110,  2785,  4348,  1112, 17154, 14570,\n",
       "           119,  1448, 14413,   131,  1103,  4106, 16133,  1204, 10475,  1518,\n",
       "           113,  5048,  7782,   178, 22221,  2956,   114,  1110,  1593, 15445,\n",
       "           117,  1133,  1117,  1762,  1110,  1107,  1103,  1268,  1282,   119,\n",
       "          1573,  1155,  1107,  1155,   117,   170,  7310,  2523,   119,   146,\n",
       "          1660,  1122,  2570,  2940,  1105,  2810,  1115, 18361,   117,  2490,\n",
       "          1209,  1267,  1103,  9304,  7956, 19425,  1107,  1103,  1273,   112,\n",
       "           188,  1436, 13129,   117,  1103,  4995, 16660, 21530,  2268,  1141,\n",
       "           119, 26556,  1268,  1208,  1112,   146,  1341,  1164,  1122,   119,\n",
       "          1406,   120,  1275,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask_rejected': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну а дальше остаётся только прожать конфиг. Они сохранены у меня в отдельных файлах, так что нужно их оттуда достать. Я по итогу отказался от использования адаптера, потому что модель не обучена вся, но опция такая есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import os\n",
    "\n",
    "peft_params = OmegaConf.to_container(\n",
    "    OmegaConf.load(Path(CONFIG_DIR, \"reward/peft.yaml\"))\n",
    ")[\"peft\"]\n",
    "peft_config = LoraConfig(**peft_params)\n",
    "\n",
    "reward_trainer_params = OmegaConf.to_container(\n",
    "    OmegaConf.load(Path(CONFIG_DIR, \"reward/trainer.yaml\"))\n",
    ")[\"trainer\"]\n",
    "reward_config = RewardConfig(**reward_trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': 'outputs/',\n",
       " 'per_device_train_batch_size': 16,\n",
       " 'num_train_epochs': 2,\n",
       " 'gradient_checkpointing': True,\n",
       " 'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
       " 'learning_rate': 2e-05,\n",
       " 'report_to': 'none',\n",
       " 'remove_unused_columns': False,\n",
       " 'optim': 'adamw_torch',\n",
       " 'logging_steps': 500,\n",
       " 'max_length': 512,\n",
       " 'load_best_model_at_end': True,\n",
       " 'evaluation_strategy': 'steps'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_trainer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускать я конечно ничего не буду, если очень хочется, то можно сделать вот так, но предварительно стоит пройтись глазами по конфигам, если что-то не устраивает - поменять. Сейчас там стоит то, с чем обучался я. Предварительно конечно стоит поставить `poetry`, но можно и убрать `!poetry run`, если есть все зависимости, то оно запустится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python warp/train_reward.py --config-name reward_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_config,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=reward_dataset[\"train\"],\n",
    "    eval_dataset=reward_dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
